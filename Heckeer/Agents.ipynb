{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "# from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from langchain.tools import Tool\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process, Queue\n",
    "from difflib import unified_diff\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = ChatOllama(\n",
    "    model=\"llama 3.1\",\n",
    "    callback_manager=callback_manager,\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id a7197595-f651-43fb-bbc7-159837a937e3\n",
      "[Document(id_='295d76f6-2cd2-406e-bbec-4e5d4353b185', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Next-Gen Dynamic Hand Gesture Recognition: MediaPipe, Inception-v3 and LSTM-Based Enhanced Deep Learning Model\\n\\nYaseen1, Oh-Jin Kwon1,*, Jaeho Kim2, Sonain Jamil3, Jinhee Lee1 and Faiz Ullah1\\n\\n1Department of Electronics Engineering, Sejong University, Seoul 05006, Republic of Korea; yaseen@sju.ac.kr (Y.); jinee5025@sju.ac.kr (J.L.); faiz@sju.ac.kr (F.U.)\\n\\n2Department of Electrical Engineering, Sejong University, Seoul 05006, Republic of Korea; kimjh@sejong.ac.kr\\n\\n3Department of Computer Science, Norwegian University of Science and Technology (NTNU), 2815 Gjovik, Norway; sonainj@stud.ntnu.no\\n\\n*Correspondence: ojkwon@sejong.ac.kr\\n\\n# Abstract\\n\\nGesture recognition is crucial in computer vision-based applications, such as drone control, gaming, virtual and augmented reality (VR/AR), and security, especially in human–computer interaction (HCI)-based systems. There are two types of gesture recognition systems, i.e., static and dynamic. However, our focus in this paper is on dynamic gesture recognition. In dynamic hand gesture recognition systems, the sequences of frames, i.e., temporal data, pose significant processing challenges and reduce efficiency compared to static gestures. These data become multi-dimensional compared to static images because spatial and temporal data are being processed, which demands complex deep learning (DL) models with increased computational costs. This article presents a novel triple-layer algorithm that efficiently reduces the 3D feature map into 1D row vectors and enhances the overall performance. First, we process the individual images in a given sequence using the MediaPipe framework and extract the regions of interest (ROI). The processed cropped image is then passed to the Inception-v3 for the 2D feature extractor. Finally, a long short-term memory (LSTM) network is used as a temporal feature extractor and classifier. Our proposed method achieves an average accuracy of more than 89.7%. The experimental results also show that the proposed framework outperforms existing state-of-the-art methods.\\n\\n# Keywords\\n\\ndynamic hand gesture recognition; hybrid deep learning; dimensionality reduction; temporal data classification; transfer learning; vision-based drone control\\n\\n# 1. Introduction\\n\\nHand gesture recognition (HGR) is a popular research area due to its scope in applications like human–computer interaction (HCI), drone control, virtual/augmented reality, sign language interpretation, and other interaction-based systems [1,2]. Gestures are natural and intuitive to communicate, sometimes with barely apparent body movements [3]. Computer vision exploits these gestures, making it possible to interact with technology without aided sensors. These gestures can be of two types: static gestures and dynamic gestures. Static gestures represent a still hand gesture or hand posture in a single image frame, i.e., relying fundamentally on spatial information [4]. Dynamic hand gestures are represented by continuous hand motion and are therefore represented by sequences of images, i.e., by temporal information [5]. Compared to static gestures, dynamic gestures can represent and enable a wider range of activities [6]. Vision-based hand gestures provide a touchless interface, and the gestures are recorded by an RGB camera device [7].\\n\\nDeep learning algorithms, like convolutional neural networks (CNN), are used for hand gesture recognition. This method recognizes hand gestures with high accuracy without using any aided sensors [8,9]. Thus, the user can interact with a computer system more naturally.\\n\\nCitation: Yaseen; Kwon, O.-J.; Kim, J.; Jamil, S.; Lee, J.; Ullah, F. Next-Gen Dynamic Hand Gesture Recognition: MediaPipe, Inception-v3 and LSTM-Based Enhanced Deep Learning Model. Electronics 2024, 13, 3233. https://doi.org/10.3390/electronics13163233\\n\\nAcademic Editors: Fath U Min Ullah, Estefanía Talavera and Nuno Gonçalves\\n\\nReceived: 12 July 2024\\n\\nRevised: 6 August 2024\\n\\nAccepted: 12 August 2024\\n\\nPublished: 15 August 2024\\n\\nCC BY Copyright: © 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cd4ab895-b6ec-4de1-b6a8-f2fd20618775', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Electronics 2024, 13, 3233\\n\\nResearchers face several challenges, and they have been addressed quite well. However, one common challenge related to dynamic hand gestures is the variability in the length of sequences of image frames in a gesture clip. Variable-length gesture clips pose challenges in data processing, model training, and inference. This also increases the computational time and power. While work has been conducted on achieving high accuracy, less attention has been given to handling the complex shape of gestures. Several authors have reported their work relating to dynamic hand gestures. Recently, Karsh et al. [4] reported their work on the same topic. However, they have tried to focus more on improving the deep learning architecture first rather than on data processing. The authors in [10] have reported work on skeletal-based hand data. For more realistic environments and practical scenarios, datasets need to be more diverse, incorporating images taken in a natural environment. Authors in [3] have also contributed to improving the accuracy by proposing a hybrid CNN model. Their hybrid model consists of a feature extractor 3D CNN module and a classifier module making use of the computational cost and power.\\n\\nOverall, the work conducted so far on dynamic hand gestures is significant, but little focus has been given to the complexity of their temporal aspects. Considering the research gap, we list our contributions as follows:\\n\\n- Extending the work conducted by authors [3], we reduce the dimensionality of the 3D feature map to 1D while retaining the temporal aspects of the data.\\n- Temporal data that is present in the sequence of frames are utilized efficiently by the proposed hybrid model at a lower computational cost than existing methods.\\n- A lightweight model is proposed, reducing the computation complexity.\\n- The performance is improved compared to the baseline algorithm. [3]\\n\\nIn summary, we address the temporal complexity of dynamic hand gestures, paving the path for a vision-based interaction system for dynamic hand gesture recognition. The rest of the paper is described as follows: In Section 2, we discuss related work and methods; Section 3 presents our proposed method; Section 4 evaluates the model and presents the results. Section 5 includes our discussion, and finally, in the Section 6 conclusions, we point out the limitations, other applications for our method, and future works.\\n\\n# 2. Related Work\\n\\nIn deep learning (DL), CNNs are used simultaneously for feature extraction and classification. Using convolution and pooling optimizes feature extraction and eases the classification process [11]. CNNs were first used in 1990 to recognize handwritten digits. The foundation of modern CNN architecture was laid down by authors [12]. Based on their architecture, more robust classification algorithms were proposed, including VGGNet [13], Resnet50 [14], and GoogleNet [15]. With the increased depth of these CNN networks, their accuracy improved, and so did their demand for high computational power. Currently, researchers are trying to build lightweight CNN models for use in devices with low computing power and in real-world scenarios [16].\\n\\n# 2.1. Dynamic Hand Gesture Classification\\n\\nThere are various deep learning architectures for temporal data classification. Some of them are worth mentioning [17], such as 2D CNNs, which are computationally less expensive and are widely used for spatial data feature extraction and classification. The more complex model architectures are 3D CNNs; the third dimension comes from temporal data in these models. Then, two-stream CNNs use 2D CNNs for sparse data classification and recurrent neural networks (RNNs) for temporal data classification. While there are several others, the hybrid approaches are the most popular as they increase accuracy while reducing the computational cost. In hybrid models, CNNs are combined with RNN, gated recurrent neural networks, or long short-term memory (LSTM) [18]. In the domain of hand gesture recognition, there exist numerous state-of-the-art methods [19–21]. Dynamic hand gestures are more complex to process as the temporal data have to be considered, as well as', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2f37988c-58bc-438a-b4e0-aff1cc507b30', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Electronics 2024, 13, 3233\\n\\nthe optical flow of the data [22]. While dynamic hand gesture recognition authors propose several novel methods, we will discuss the most recent hybrid methods.\\n\\nAuthors in [23] investigated HGR with leap motion sensor-based data, and although they achieved an accuracy of 97%, the work does not provide a fully touchless interface as in the case of RGB data. This work was further extended by Huu [24] while using two-stream approaches, achieving a similar accuracy [23]. In the case of 3D CNNs, authors in the work [25] claimed an accuracy of 90%, but their system is limited to working only under the condition that a person has to sit close to the camera while performing gestures. Another limitation is that they use more complex CNN models for their work. Similarly, in the case of hybrid approaches, another hybrid model was recently proposed by authors [3]. The authors used Google’s Inception-v3 architecture as a feature extractor and LSTM for temporal data feature extraction and classification. They have achieved an overall accuracy of 84%. However, their work could not process variable-length sequences of dynamic hand gestures, and there is more room for improvement in the accuracy [3]. Even if the accuracy was much higher, the practical use of this model in real-world scenarios is limited, as during live feed from the camera, the length of a gesture clip is always variable [26], which the authors couldn’t address in their work [3].\\n\\n# 2.2. Classification with Hybrid Deep Learning\\n\\nAs we know, for a dynamic hand gesture recognition system, spatial data processing is not sufficient; temporal relations in sequences of frames have to be exploited [27]. Various state-of-the-art methods have been reported to work on video classification tasks, especially on dynamic hand gestures using hybrid approaches, i.e., combining CNN or RNN, CNN, and LSTM, and similar other approaches [1]. Various studies have tried exploiting hybrid deep learning models based on CNNs and RNNs for HGR [24,28] and other video classification tasks [29].\\n\\nIn traditional neural networks, neurons are only connected in a feed-forward direction, while in an RNN, the neurons are not only connected in a mixed fashion within a layer but are also connected in opposite directions (both forward and backward) [30]. Furthermore, LSTMs are modified RNNs for the classification of temporal data so that they can store the data in memory cells. These cells contain the output of the previous state relative to the current input. Through memory cells, the temporal relation between the data can be retained [31].\\n\\nFigure 1 shows this hybrid approach; CNN is used as a feature extractor as they are suitable for recognizing image patterns. At this stage, the spatial information from individual images is only processed using 2D CNN, which does not classify but obtains the hidden patterns. The feature maps for all the frames within the sequence are then passed into the LSTM network. The LSTM extracts features and hidden patterns, exploring temporal relations from the feature maps and performing classification. This yields improved classification and better results [32].\\n\\n|Feature Extractor (2D CNNs)|Classification|\\n|---|---|\\n|2D feature maps|Feature Extraction|\\n|frames|LSTM/RNNs|\\n|Spatial data|Temporal data|\\n\\nFigure 1. A basic CNN-RNN-based hybrid architecture.\\n\\n# 3. Methodology\\n\\nCompared to static hand gestures, dynamic hand gestures have sequences of frames where the hand moment and position within each frame are recorded concerning the time. For this reason, the hybrid deep neural network architecture that comprises CNN and RNN is well suited for temporal data classification tasks [33]. Our proposed architecture', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c94bccdb-2412-449d-aa0d-86c94eff38ac', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Electronics 2024, 13, 3233\\n\\nis inspired by Hax et al. [3]. They have used the Google Inception v3 [34] model for feature extraction and an LSTM for classification. In our case, we have replaced the Google Inception v3 [34] with MediaPipe 0.10.14 in the input layer [35]—to remove unwanted features from the frame and locate the hand gesture efficiently. MediaPipe has a built-in hand landmark model, which returns the knuckle points of the hands in an image with low computational power. Using this advantage, we locate the region of the hand in an image and crop the area called the region of interest (ROI). This cropped image is used in the input for the Inception-v3 layer of feature extraction. The highly efficient approach leads to improved accuracy and lightweight models, reducing the computation complexity and cost. The following points are presented for comparison:\\n\\n- The use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the image dimension and computational cost.\\n- It also offers improved efficiency for hand landmark detection compared to Inception v3 alone.\\n- Time complexity is low, and real-time performance is highly durable compared to Inception v3 without MediaPipe.\\n- MediaPipe, when employed as an ROI extractor on the input layer in a hybrid architecture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement in terms of classification accuracy.\\n\\nComparing our architecture to the baseline, we have proposed the following changes:\\n\\n- We have moved Google Inception v3 from the input layer to the middle layer [34]. The input layer uses Google MediaPipe as an ROI extractor [35].\\n- The frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n- In the case of baseline architecture, ten frames are processed per sequence, and the proposed architecture can process variable-length sequences.\\n\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2. The LSTM exploits the temporal relationship of the received data for classification. We replicate the same layer architecture as used by the authors [3], including dropout layers and a dense layer (using a softmax function) at the final stage to prevent the model from over-fitting and classification, respectively. The proposed hybrid model consists of three main layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer, Google Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM layer, which is used as a classifier [18].\\n\\n# 3.1. Dynamic Hand Gesture Dataset\\n\\nFor a fair comparison of our proposed work with that of the baseline architecture, we select the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by the authors [3]. This dataset has been collected concerning different subjects under various lighting conditions. It has six different gestures, each with 662 sequences, comprising 40 frames per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left, scroll right, zoom in, and zoom out. The background has additional people sometimes engaging in other activities; this is included intentionally to add complexity to the dataset for real-world simulation. A depth camera recorded the dataset so that it also contains depth data, but our focus in this paper is a vision-based hybrid model.\\n\\n# 3.2. Data Processing Pipeline\\n\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in Python using TensorFlow. The proposed model consists of three main blocks: MediaPipe, Inception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing stage, frames from the directory are loaded sequence by sequence, and every fourth frame is selected from the sequence, as more frames did not increase the accuracy [3]. Finally, a total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of interest (ROI) extractor. The ROI is cropped for each frame. This technique removes the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='843d57dd-89a3-4213-8bf4-8a21d6692263', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Electronics 2024, 13, 3233\\n\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from 1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from the MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based feature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048, 1) per sequence. This data frame is then used as an input to the LSTM block for final feature extraction and classification. Also, 70, 20, and 10% of the data is split into training, validation, and testing portions.\\n\\n# Hybrid Feature Extractor\\n\\n|Input layer:50x60x3|FlF2 F3|FN|\\n|---|---|---|\\n|Conv Layer|X| |\\n|Max Pooling|F| |\\n|Inception Layers| | |\\n|Average layers| | |\\n|Concatenation|LSTM|(tanh)|\\n|(2048 X 1)|Dense|(ReLU)|\\n|Cropped image|(512 X 1)| |\\n|MediaPipe|Dense|(softmax)|\\n|ROL Extractor|(6 X 1)| |\\n\\nInput Image:1600 X 900 X 3\\n\\nZoom out\\n\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n\\n# 3.3. Evaluating Criteria\\n\\nFor the model’s assessment and evaluation, we used the following metrics:\\n\\n- Accuracy: Reflects the model’s performance regarding the overall correctness for the given number of test instances (1).\\n- Recall: Also called true positive rate as it measures the model’s ability regarding true positives (2).\\n- F1-score: Reflects the model’s ability concerning false positives and negatives (3).\\n- Specificity: Reflects the model’s ability to correctly identify true negative instances (4).\\n\\nFormulas for each of these metrics are given below:\\n\\n|Accuracy =|TP + TN|(1)|\\n|---|---|---|\\n| |TP + TN + FP + FN| |\\n|Recall =|TP|(2)|\\n| |TP + FN| |\\n|F1-score =|2 × Precision × Recall|(3)|\\n| |Precision + Recall| |\\n|Specificity =|TN|(4)|\\n| |TN + FP| |', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4eb8d358-bbcc-4182-aeca-0cdfa5a70ce6', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Results based on the above metrics are shown in Table 1. For more visually appealing results and deeper insights, we have used a confusion matrix as shown in Figures 3 and 4. Also, to further evaluate our results for the proposed architecture, we replace the LSTM classifier with a gated recurrent unit (GRU) as these are also the most promising models [18]. We consider the same data split for this architecture as well. We have also used the Resnet50, replacing Inception v3 in our architecture, for further comparison.\\n\\n# Figure 3. Confusion matrix based on validation data.\\n\\n|down|120|\\n|---|---|\\n|left|118|\\n|right|123|\\n|scroll|121|\\n|zoom|119|\\n\\n# Figure 4. Confusion matrix based on test data.\\n\\n|down|57|\\n|---|---|\\n|left|61|\\n|right|59|\\n|scroll|60|', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='646e82c8-1677-41a2-ab34-758754223cd3', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Electronics 2024, 13, 3233\\n\\n# 4. Results\\n\\nThis Section presents the experimental setup, performance metrics, and visualization (the reason and rationale behind our model architecture and dataset pre-processing).\\n\\n# 4.1. Experimental Setup\\n\\nThe models were trained for 300 epochs, with early stopping enabled to avoid over-fitting, and the initial learning rate was set to 0.001. The categorical cross-entropy cost function was used for training and validation losses for maximum learning. The adam-optimizer was used to iteratively update and adjust the network weights in order to minimize these losses. In the case of activation functions, the tangent hyperbolic function was used for the LSTM layer and the softmax function for the final output layer. For the intermediate layer (Dense layer), the rectified linear unit (RelU) was employed (see Figure 2).\\n\\nThe machine we used for our pipeline has the following specs: We used an HP desktop computer, manufactured by HP Inc., Palo Alto, CA, USA, model Pavilion Gaming Desktop TG01-1xxx, with a RAM of 32 GB, GPU of NVIDIA GeForce RTX 2060 SUPER, and eight cores with clock speed of 2.9 GHz.\\n\\n# 4.2. Performance Metrics and Visualizations\\n\\nThe proposed hybrid deep learning-based model for hand gesture recognition demonstrated good performance when evaluated via various metrics, as shown in Table 1. The empirical tests, both on validation and test results, showcase satisfactory results; an average accuracy of 89.7% on test data and 90.1% on validation data was reported. Other metrics were also used to assess the model’s ability; we achieved a recall of 91%, an F1-score of 90.0%, and a specificity of 98.0%.\\n\\nThe results were also compared to that of other models [3,37–39], as shown in Table 2. The average accuracy on the test, as well as the validation data, reveal a clear win for our proposed architecture. The Hax et al. [3] model has an average accuracy of 84.7%, while ours achieved 90.1%. Similarly, for test data, the Hax et al. [3] model achieved an accuracy of 83.6%, while ours achieved an average accuracy of 89.7%.\\n\\n**Table 1. Average results based on the performance metrics for test and validation data.**\\n|Data|Recall|F1-Score|Specificity|Accuracy|\\n|---|---|---|---|---|\\n|Validation|89.9%|89.7%|98.0%|90.1%|\\n|Test|91.5%|90.0%|97.9%|89.7%|\\n\\n**Table 2. Average results based on the performance metrics for test and validation data.**\\n|Architecture|Model|Data|Accuracy|\\n|---|---|---|---|\\n|Hax et al. [3]|Inception-LSTM|Validation|84.7%|\\n| | |Test|83.6%|\\n|Res3-ATN [37]|Residual-Attention Network|Validation|64.13%|\\n| | |Test|62.70%|\\n|Spat. st. CNN [38]|Two Streams CNN|Validation|55.79%|\\n| | |Test|54.60%|\\n|C3D [39]|3D CNNs|Validation|64.90%|\\n| | |Test|69.30%|\\n| |ROI-Inception-LSTM|Validation|90.1%|\\n| | |Test|89.7%|\\n| |ROI-Inception-GRU|Validation|85.6%|\\n| | |Test|84.1%|\\n\\nFigures 3 and 4 also present the classification confusion data for each category dataset. The true positive rate for each category is given on the diagonal squares in the confusion matrix. The most precise detected moment in validation is the “scroll_right” category.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3a1ab53c-d17d-454c-bcfd-2c003a0bf2f7', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Electronics 2024, 13, 3233\\n\\nThe confusion matrix also gives a detailed insight into the true negatives, false positives, and false negatives for each of the categories. By comparing these results with the baseline, our proposed method demonstrates excellent performance, which can be attributed to our modification to the proposed architecture.\\n\\nWe also compared the performance of our model with the state-of-the-art models. The classification results, as shown in Table 2, were quite satisfactory compared to other models. To compare the computational complexity and memory usage of the proposed model with the baseline methods, we have provided a comparison in Table 3. The results clearly show the superiority of the proposed model in terms of weight and low memory usage during inference.\\n\\n**Table 3. Comparison of model parameters and memory size.**\\n|Model|Parameters|VRAM (MB)|\\n|---|---|---|\\n|Hax et. al. [3]|29 M|450|\\n|Res3-ATN [37]|26 M|400|\\n|Spat. st CNN [38]|38 M|600|\\n|C3D [39]|25 M|350|\\n|Ours|16 M|150|\\n\\n# 5. Discussion\\n\\nSection 4.2 is quite revealing in many ways, as it presents the detailed average results in Tables 1 and 2 and the confusion matrix; the results show uniform error properties across all categories. Gestures like “zoom_in” and “zoom_out” are primarily confused. Still, the true positive rate is above 83%. The Inception-v3 combined with MediaPipe is a booster for enhanced dynamic hand gesture classification. Even though the data were scarce, the model performance demonstrated that this algorithm can perform much better on sufficient data.\\n\\nThe proposed model can also be used in other areas of HCI where temporal classification is demanded with minimal data availability for training. As discussed in the related work, in the classification area using CNNs, the trend is to use a lightweight architecture that could achieve better results for practical application scenarios. Our proposed model paved the path for more lightweight architecture. It uses MediaPipe to extract the regions most wanted from the image (ROI) and feed them to the middle layer for feature extraction. This approach reduces the dimensionality of data and increases the availability of rich features; the LSTM layer further utilizes this for temporal data extraction and classification.\\n\\nOur proposed model can also be used in lightweight real-world applications like drone control where real-time classification is needed. This is because drone control is usually an outdoor activity, and the model can extract rich features with the power of the MediaPipe and Inception-v3 layer together, as well as being able to perform better in the case of dynamic gestures when LSTM is used in the top layer as a classifier and temporal feature extractor. The deep learning models are always data-hungry, which is why the current trend is to use transfer learning to overcome the issue of data scarcity. Using Resnet50, Inception-v3 is one of the many modules used in deep learning models to mitigate the data scarcity issue; the proposed architecture can be used as a transfer learning model in dynamic hand gesture recognition in various HCI applications.\\n\\n# 6. Conclusions\\n\\nIn this work, we proposed a hybrid deep learning architecture for classifying dynamic hand gestures (video clips) for use in real-world scenarios. The architecture comprises three layers: The MediaPipe-based ROI extractor is the bottom layer, the middle layer uses 2D CNN (Inception v3) as a feature extractor, and the top layer is a temporal feature extractor and classifier. The proposed model was trained by the Depth_Camera_dataset, a dataset designed explicitly for dynamic hand gestures [36]. With our best-split data, the model achieved an average test accuracy of more than 89.7%. This can be attributed to our novel changes in the proposed hybrid architecture, which includes the addition of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='adf4f4cf-561a-443c-b0e8-4f7c5b5d936e', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Electronics 2024, 13, 3233\\n\\nthe bottom layer, the MediaPipe-based hand region extractor (ROI), which reduces the dimensionality and computational cost. The second reason is that the feature extractors were doubled; Inception v3 was used as a feature extractor in a focused area, enhancing the features’ quality while reducing the computational cost. The third change we made was the input to the LSTM layer, which concatenates features obtained for a sequence of frames per gesture. In this study, we focused on limited gestures that were available in the dataset; further investigations can be carried out on datasets with a large number of gestures. Also, for real world applications, diversity in the datasets is mandatory, as well as the subjective evaluation of the model. Future work includes the subjective testing of the proposed architecture on other benchmark datasets and training the model on variable-length hand gesture clips.\\n\\n# Author Contributions:\\n\\nConceptualization, Y. and O.-J.K.; data curation, Y.; formal analysis, Y.; funding acquisition, O.-J.K., J.K. and J.L.; project administration, J.L.; resources, J.L.; software, Y., J.L. and F.U.; supervision, O.-J.K.; visualization, Y.; writing—original draft, Y. and S.J.; writing—review and editing, Y. and S.J. All authors have read and agreed to the published version of the manuscript.\\n\\n# Funding:\\n\\nThis research was supported by Unmanned Vehicles Core Technology Research and Development Program through the National Research Foundation of Korea (NRF) and Unmanned Vehicle Advanced Research Center (UVARC) funded by the Ministry of Science and ICT, the Republic of Korea (2023M3C1C1A01098414). This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2024-2021-0-01816) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation).\\n\\n# Data Availability Statement:\\n\\nThe raw data supporting the conclusions of this article will be made available by the authors on request.\\n\\n# Conflicts of Interest:\\n\\nThe authors declare no conflicts of interest.\\n\\n# Abbreviations\\n\\nThe following abbreviations are used in this manuscript:\\n\\n|HGR|Hand Gesture Recognition|\\n|---|---|\\n|CNN|Convolutional Neural Networks|\\n|RNN|Recurrent Neural Networks|\\n|GRU|Gated Recurrent Unit|\\n|LSTM|Long Short-Term Memory|\\n|ROI|Region of Interest|\\n\\n# References\\n\\n1. Rastgoo, R.; Kiani, K.; Escalera, S.; Sabokrou, M. Multi-modal zero-shot dynamic hand gesture recognition. Expert Syst. Appl. 2024, 247, 123349. [CrossRef]\\n2. Balaji, P.; Prusty, M.R. Multimodal fusion hierarchical self-attention network for dynamic hand gesture recognition. J. Vis. Commun. Image Represent. 2024, 98, 104019. [CrossRef]\\n3. Hax, D.R.T.; Penava, P.; Krodel, S.; Razova, L.; Buettner, R. A Novel Hybrid Deep Learning Architecture for Dynamic Hand Gesture Recognition. IEEE Access 2024, 12, 28761–28774. [CrossRef]\\n4. Karsh, B.; Laskar, R.H.; Karsh, R.K. mXception and dynamic image for hand gesture recognition. Neural Comput. Appl. 2024, 36, 8281. [CrossRef]\\n5. Sunanda; Balmik, A.; Nandy, A. A novel feature fusion technique for robust hand gesture recognition. Multimed. Tools Appl. 2024, 83, 65815–65831. [CrossRef]\\n6. Shi, Y.; Li, Y.; Fu, X.; Miao, K.; Miao, Q. Review of dynamic gesture recognition. Virtual Real. Intell. Hardw. 2021, 3, 183–206. [CrossRef]\\n7. Jain, R.; Karsh, R.K.; Barbhuiya, A.A. Literature review of vision-based dynamic gesture recognition using deep learning techniques. Concurr. Comput. Pract. Exp. 2022, 34, e7159. [CrossRef]\\n8. Kapuscinski, T.; Inglot, K. Vision-based gesture modeling for signed expressions recognition. Procedia Comput. Sci. 2022, 207, 1007–1016. [CrossRef]\\n9. Yaseen; Kwon, O.J.; Lee, J.; Ullah, F.; Jamil, S.; Kim, J.S. Automatic Sequential Stitching of High-Resolution Panorama for Android Devices Using Precapture Feature Detection and the Orientation Sensor. Sensors 2023, 23, 879. [CrossRef]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ec104412-8533-4dba-98c7-ee7e87726611', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Electronics 2024, 13, 3233\\n\\n# References\\n\\n1. Abdullahi, S.B.; Chamnongthai, K. American sign language words recognition of skeletal videos using processed video driven multi-stacked deep LSTM. Sensors 2022, 22, 1406. [CrossRef] [PubMed]\\n2. LeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature 2015, 521, 436–444. [CrossRef] [PubMed]\\n3. Saxe, A.; Nelli, S.; Summerfield, C. If deep learning is the answer, what is the question? Nat. Rev. Neurosci. 2021, 22, 55–67. [CrossRef] [PubMed]\\n4. Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv 2014, arXiv:1409.1556.\\n5. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June–1 July 2016; pp. 770–778.\\n6. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7–12 June 2015; pp. 1–9.\\n7. Li, Z.; Liu, F.; Yang, W.; Peng, S.; Zhou, J. A survey of convolutional neural networks: Analysis, applications, and prospects. IEEE Trans. Neural Networks Learn. Syst. 2021, 33, 6999–7019. [CrossRef] [PubMed]\\n8. Wang, S.; Cao, J.; Philip, S.Y. Deep learning for spatio-temporal data mining: A survey. IEEE Trans. Knowl. Data Eng. 2020, 34, 3681–3700. [CrossRef]\\n9. Ur Rehman, A.; Belhaouari, S.B.; Kabir, M.A.; Khan, A. On the use of deep learning for video classification. Appl. Sci. 2023, 13, 2007. [CrossRef]\\n10. Adithya, V.; Rajesh, R. A deep convolutional neural network approach for static hand gesture recognition. Procedia Comput. Sci. 2020, 171, 2353–2361.\\n11. Xia, C.; Saito, A.; Sugiura, Y. Using the virtual data-driven measurement to support the prototyping of hand gesture recognition interface with distance sensor. Sens. Actuators A Phys. 2022, 338, 113463. [CrossRef]\\n12. Dang, T.L.; Tran, S.D.; Nguyen, T.H.; Kim, S.; Monet, N. An improved hand gesture recognition system using keypoints and hand bounding boxes. Array 2022, 16, 100251. [CrossRef]\\n13. Rautaray, S.S.; Agrawal, A. Real time gesture recognition system for interaction in dynamic environment. Procedia Technol. 2012, 4, 595–599. [CrossRef]\\n14. Naguri, C.R.; Bunescu, R.C. Recognition of dynamic hand gestures from 3D motion data using LSTM and CNN architectures. In Proceedings of the 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), Cancun, Mexico, 18–21 December 2017; pp. 1130–1133.\\n15. Huu, P.N.; Ngoc, T.L. Two-stream convolutional network for dynamic hand gesture recognition using convolutional long short-term memory networks. Vietnam J. Sci. Technol. 2020, 58, 514–523.\\n16. Zhang, W.; Wang, J. Dynamic hand gesture recognition based on 3D convolutional neural network models. In Proceedings of the 2019 IEEE 16th International Conference on Networking, Sensing and Control (ICNSC), Banff, AB, Canada, 9–11 May 2019; pp. 224–229.\\n17. Wang, X.; Lafreniere, B.; Zhao, J. Exploring Visualizations for Precisely Guiding Bare Hand Gestures in Virtual Reality. In Proceedings of the CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, 11–14 May 2024; pp. 1–19.\\n18. Ye, W.; Cheng, J.; Yang, F.; Xu, Y. Two-stream convolutional network for improving activity recognition using convolutional long short-term memory networks. IEEE Access 2019, 7, 67772–67780. [CrossRef]\\n19. Ma, C.Y.; Chen, M.H.; Kira, Z.; AlRegib, G. TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition. Signal Process. Image Commun. 2019, 71, 76–87. [CrossRef]\\n20. Abdullahi, S.B.; Bature, Z.A.; Gabralla, L.A.; Chiroma, H. Lie recognition with multi-modal spatial–temporal state transition patterns based on hybrid convolutional neural network–bidirectional long short-term memory. Brain Sci. 2023, 13, 555. [CrossRef] [PubMed]\\n21. Durstewitz, D.; Koppe, G.; Thurm, M.I. Reconstructing computational system dynamics from neural data with recurrent neural networks. Nat. Rev. Neurosci. 2023, 24, 693–710. [CrossRef] [PubMed]\\n22. Hendrikx, N.; Barhmi, K.; Visser, L.; de Bruin, T.; Pó, M.; Salah, A.; van Sark, W. All sky imaging-based short-term solar irradiance forecasting with Long Short-Term Memory networks. Sol. Energy 2024, 272, 112463. [CrossRef]\\n23. Rafiq, I.; Mahmood, A.; Ahmed, U.; Khan, A.R.; Arshad, K.; Assaleh, K.; Ratyal, N.I.; Zoha, A. A Hybrid Approach for Forecasting Occupancy of Building’s Multiple Space Types. IEEE Access 2024, 12, 50202–50216. [CrossRef]\\n24. Pan, Y.; Shang, Y.; Liu, T.; Shao, Z.; Guo, G.; Ding, H.; Hu, Q. Spatial–temporal attention network for depression recognition from facial videos. Expert Syst. Appl. 2024, 237, 121410. [CrossRef]\\n25. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; Wojna, Z. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June–1 July 2016; pp. 2818–2826.\\n26. Lugaresi, C.; Tang, J.; Nash, H.; McClanahan, C.; Uboweja, E.; Hays, M.; Zhang, F.; Chang, C.L.; Yong, M.; Lee, J.; et al. Mediapipe: A framework for perceiving and processing reality. In Proceedings of the Third Workshop on Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition (CVPR) 2019, Long Beach, CA, USA, 17 June 2019; Volume 2019.\\n27. Jeeru, S.; Sivapuram, A.K.; León, D.G.; Gröli, J.; Yeduri, S.R.; Cenkeramaddi, L.R. Depth camera based dataset of hand gestures. Data Brief 2022, 45, 108659. [CrossRef]\\n28. Dhingra, N.; Kunz, A. Res3atn-Deep 3D residual attention network for hand gesture recognition in videos. In Proceedings of the 2019 International Conference on 3D Vision (3DV), Quebec City, QC, Canada, 16–19 September 2019; pp. 491–501.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='40695bc0-6e67-45c1-bf53-140a51b5f858', embedding=None, metadata={'file_path': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf', 'file_name': 'electronics-13-03233.pdf', 'file_type': 'application/pdf', 'file_size': 957053, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Electronics 2024, 13, 3233\\n\\n# References\\n\\n1. Simonyan, K.; Zisserman, A. Two-stream convolutional networks for action recognition in videos. In Proceedings of the Advances in Neural Information Processing Systems, Montreal, QC, Canada, 8–13 December 2014; Volume 27.\\n2. Tran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; Paluri, M. Learning spatiotemporal features with 3D convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision 2015, Santiago, Chile, 7–13 December 2015; pp. 4489–4497.\\n\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "LLAMA_CLOUD_API_KEY=\"llx-nxrF5SQ5MnbMrKhrb1HqfdFA3YajxTDjtjfctsyebDSdBa7W\"\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMA_CLOUD_API_KEY,\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "\n",
    "# use SimpleDirectoryReader to parse our file\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_files=['/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'], file_extractor=file_extractor).load_data()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://:\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"Hecker\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    # embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def chunk_text_by_sentence(text, chunk_size=2048):\n",
    "    \"\"\"Chunk the $text into sentences with less than 2k tokens.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunked_text = []\n",
    "    curr_chunk = []\n",
    "    for sentence in sentences:\n",
    "        if num_tokens_from_string(\". \".join(curr_chunk)) + num_tokens_from_string(sentence) + 2 <= chunk_size:\n",
    "            curr_chunk.append(sentence)\n",
    "        else:\n",
    "            chunked_text.append(\". \".join(curr_chunk))\n",
    "            curr_chunk = [sentence]\n",
    "    if curr_chunk:\n",
    "        chunked_text.append(\". \".join(curr_chunk))\n",
    "    return chunked_text[0]\n",
    "\n",
    "def chunk_text_front(text, chunk_size = 2048):\n",
    "    '''\n",
    "    get the first `trunk_size` token of text\n",
    "    '''\n",
    "    chunked_text = \"\"\n",
    "    tokens = num_tokens_from_string(text)\n",
    "    if tokens < chunk_size:\n",
    "        return text\n",
    "    else:\n",
    "        ratio = float(chunk_size) / tokens\n",
    "        char_num = int(len(text) * ratio)\n",
    "        return text[:char_num]\n",
    "\n",
    "def chunk_texts(text, chunk_size = 2048):\n",
    "    '''\n",
    "    trunk the text into n parts, return a list of text\n",
    "    [text, text, text]\n",
    "    '''\n",
    "    tokens = num_tokens_from_string(text)\n",
    "    if tokens < chunk_size:\n",
    "        return [text]\n",
    "    else:\n",
    "        texts = []\n",
    "        n = int(tokens/chunk_size) + 1\n",
    "        \n",
    "        part_length = len(text) // n\n",
    "        \n",
    "        extra = len(text) % n\n",
    "        parts = []\n",
    "        start = 0\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            end = start + part_length + (1 if i < extra else 0)\n",
    "            parts.append(text[start:end])\n",
    "            start = end\n",
    "        return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def get_draft(question):\n",
    "    # Getting the draft answer\n",
    "    draft_prompt = '''\n",
    "IMPORTANT:\n",
    "Try to answer this question/instruction with step-by-step thoughts and make the answer more structural.\n",
    "Use `\\n\\n` to split the answer into several paragraphs.\n",
    "Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "    # openai_client = OpenAI(api_key=openai.api_key)\n",
    "    draft_agent = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    response = draft_agent.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"{question}\\n{draft_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import tiktoken\n",
    "from typing import List, Optional\n",
    "\n",
    "class DocumentParser:\n",
    "    def __init__(self, result_type=\"markdown\"):\n",
    "        self.parser = LlamaParse(result_type=result_type)\n",
    "        self.file_extractor = {\".pdf\": self.parser}\n",
    "        \n",
    "    def load_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Load and parse document\"\"\"\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_files=[file_path], \n",
    "            file_extractor=self.file_extractor\n",
    "        ).load_data()\n",
    "        return [doc.text for doc in documents]\n",
    "    \n",
    "    def chunk_document(self, content: str, chunk_size: int = 2048) -> List[str]:\n",
    "        \"\"\"Chunk document content\"\"\"\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        tokens = encoding.encode(content)\n",
    "        chunks = []\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for token in tokens:\n",
    "            if current_size + 1 <= chunk_size:\n",
    "                current_chunk.append(token)\n",
    "                current_size += 1\n",
    "            else:\n",
    "                chunks.append(encoding.decode(current_chunk))\n",
    "                current_chunk = [token]\n",
    "                current_size = 1\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunks.append(encoding.decode(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def parse_and_chunk(self, file_path: str) -> Optional[List[str]]:\n",
    "        \"\"\"Parse document and return chunks\"\"\"\n",
    "        try:\n",
    "            content = self.load_document(file_path)\n",
    "            if not content:\n",
    "                return None\n",
    "            \n",
    "            all_chunks = []\n",
    "            for text in content:\n",
    "                chunks = self.chunk_document(text)\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            return all_chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing document: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_draft(draft, split_char = '\\n\\n'):\n",
    "    paragraphs = draft.split(split_char)\n",
    "    draft_paragraphs = [para for para in paragraphs if len(para)>5]\n",
    "    # print(f\"The draft answer has {len(draft_paragraphs)}\")\n",
    "    return draft_paragraphs\n",
    "\n",
    "def split_draft_openai(question, answer, NUM_PARAGRAPHS = 4):\n",
    "    split_prompt = f'''\n",
    "Split the answer of the question into multiple paragraphs with each paragraph containing a complete thought.\n",
    "The answer should be splited into less than {NUM_PARAGRAPHS} paragraphs.\n",
    "Use ## as splitting char to seperate the paragraphs.\n",
    "So you should output the answer with ## to split the paragraphs.\n",
    "**IMPORTANT**\n",
    "Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    splited_answer = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Question: {question}\\n\\n##Response: {answer}\\n\\n##Instruction: {split_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    split_draft_paragraphs = split_draft(splited_answer, split_char = '##')\n",
    "    return split_draft_paragraphs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(question, answer):\n",
    "    query_prompt = '''\n",
    "I want to verify the content correctness of the given question, especially the last sentences.\n",
    "Please summarize the content with the corresponding question.\n",
    "This summarization will be used as a query to search with Bing search engine.\n",
    "The query should be short but need to be specific to promise Bing can find related knowledge or pages.\n",
    "You can also use search syntax to make the query short and clear enough for the search engine to find relevant language data.\n",
    "Try to make the query as relevant as possible to the last few sentences in the content.\n",
    "**IMPORTANT**\n",
    "Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "    # openai_client = OpenAI(api_key = openai.api_key)\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    query = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Question: {question}\\n\\n##Response: {answer}\\n\\n##Instruction: {query_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    return query.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revise_answer(question, answer, content):\n",
    "    revise_prompt = '''\n",
    "I want to revise the answer according to retrieved related text of the question in WIKI pages.\n",
    "You need to check whether the answer is correct.\n",
    "If you find some errors in the answer, revise the answer to make it better.\n",
    "If you find some necessary details are ignored, add it to make the answer more plausible according to the related text.\n",
    "If you find the answer is right and do not need to add more details, just output the original answer directly.\n",
    "**IMPORTANT**\n",
    "Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.\n",
    "Add more details from retrieved text to the answer.\n",
    "Split the paragraphs with \\n\\n characters.\n",
    "Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.\n",
    "'''\n",
    "    # openai_client = OpenAI(api_key = openai.api_key)\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    revised_answer = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Existing Text in Wiki Web: {content}\\n\\n##Question: {question}\\n\\n##Answer: {answer}\\n\\n##Instruction: {revise_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    return revised_answer.content\n",
    "\n",
    "def get_reflect_answer(question, answer):\n",
    "    reflect_prompt = '''\n",
    "Give a title for the answer of the question.\n",
    "And add a subtitle to each paragraph in the answer and output the final answer using markdown format. \n",
    "This will make the answer to this question look more structured for better understanding.\n",
    "**IMPORTANT**\n",
    "Try to keep the structure (multiple paragraphs with its subtitles) in the response and make it more structual for understanding.\n",
    "Split the paragraphs with \\n\\n characters.\n",
    "Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.\n",
    "'''\n",
    "\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    reflected_answer = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Question:\\n{question}\\n\\n##Answer:\\n{answer}\\n\\n##Instruction:\\n{reflect_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    return reflected_answer.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_query_wrapper(q, question, answer):\n",
    "    result = get_query(question, answer)\n",
    "    q.put(result)\n",
    "\n",
    "# def get_content_wrapper(q, query):\n",
    "#     result = get_content(query)\n",
    "#     q.put(result)\n",
    "\n",
    "def get_revise_answer_wrapper(q, question, answer, content):\n",
    "    result = get_revise_answer(question, answer, content)\n",
    "    q.put(result)\n",
    "\n",
    "def get_reflect_answer_wrapper(q, question, answer):\n",
    "    result = get_reflect_answer(question, answer)\n",
    "    q.put(result)\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "def run_with_timeout(func, timeout, *args, **kwargs):\n",
    "    q = Queue()  \n",
    "    p = Process(target=func, args=(q, *args), kwargs=kwargs)\n",
    "    p.start()\n",
    "    p.join(timeout)\n",
    "    if p.is_alive():\n",
    "        print(f\"{datetime.now()} [INFO] Function {str(func)} running timeout ({timeout}s), terminating...\")\n",
    "        p.terminate()\n",
    "        p.join() \n",
    "        result = None\n",
    "    else:\n",
    "        print(f\"{datetime.now()} [INFO] Function {str(func)} executed successfully.\")\n",
    "        result = q.get()  # 从队列中获取结果\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/utils.py:1017: UserWarning: Expected 2 arguments for function <function rat at 0x7677e6b2aca0>, received 1.\n",
      "  warnings.warn(\n",
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/utils.py:1021: UserWarning: Expected at least 2 arguments for function <function rat at 0x7677e6b2aca0>, received 1.\n",
      "  warnings.warn(\n",
      "Exception in thread Thread-7 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-7 (run)'.\n",
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py:1047: RuntimeWarning: coroutine 'Server.serve' was never awaited\n",
      "  self._invoke_excepthook(self)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Exception in thread Thread-8 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-8 (run)'.\n",
      "Exception in thread Thread-9 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-9 (run)'.\n",
      "Exception in thread Thread-10 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-10 (run)'.\n",
      "Exception in thread Thread-11 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-11 (run)'.\n",
      "Exception in thread Thread-12 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-12 (run)'.\n",
      "Exception in thread Thread-13 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-13 (run)'.\n",
      "Exception in thread Thread-14 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/asyncio/events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-14 (run)'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 213\u001b[0m\n\u001b[1;32m    195\u001b[0m     regenerate_btn\u001b[38;5;241m.\u001b[39mclick(\n\u001b[1;32m    196\u001b[0m         fn \u001b[38;5;241m=\u001b[39m rat,\n\u001b[1;32m    197\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m [instruction_box],\n\u001b[1;32m    198\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m [chatgpt_box, rat_box]\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    201\u001b[0m     examples \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mExamples(\n\u001b[1;32m    202\u001b[0m         examples\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;66;03m# \"I went to the supermarket yesterday.\", \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m[instruction_box]\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 213\u001b[0m \u001b[43mdemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/blocks.py:2572\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[1;32m   2567\u001b[0m     (\n\u001b[1;32m   2568\u001b[0m         server_name,\n\u001b[1;32m   2569\u001b[0m         server_port,\n\u001b[1;32m   2570\u001b[0m         local_url,\n\u001b[1;32m   2571\u001b[0m         server,\n\u001b[0;32m-> 2572\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[0;32m~/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/http_server.py:151\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    141\u001b[0m         reloader \u001b[38;5;241m=\u001b[39m SourceFileReloader(\n\u001b[1;32m    142\u001b[0m             app\u001b[38;5;241m=\u001b[39mapp,\n\u001b[1;32m    143\u001b[0m             watch_dirs\u001b[38;5;241m=\u001b[39mGRADIO_WATCH_DIRS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m             watch_module\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    150\u001b[0m     server \u001b[38;5;241m=\u001b[39m Server(config\u001b[38;5;241m=\u001b[39mconfig, reloader\u001b[38;5;241m=\u001b[39mreloader)\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, ServerFailedToStartError):\n",
      "File \u001b[0;32m~/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/http_server.py:58\u001b[0m, in \u001b[0;36mServer.run_in_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarted:\n\u001b[0;32m---> 58\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServerFailedToStartError(\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer failed to start. Please check that the port is available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from difflib import unified_diff\n",
    "from IPython.display import display, HTML\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_diff_html(text1, text2):\n",
    "    diff = unified_diff(text1.splitlines(keepends=True),\n",
    "                        text2.splitlines(keepends=True),\n",
    "                        fromfile='text1', tofile='text2')\n",
    "\n",
    "    diff_html = \"\"\n",
    "    for line in diff:\n",
    "        if line.startswith('+'):\n",
    "            diff_html += f\"<div style='color:green;'>{line.rstrip()}</div>\"\n",
    "        elif line.startswith('-'):\n",
    "            diff_html += f\"<div style='color:red;'>{line.rstrip()}</div>\"\n",
    "        elif line.startswith('@'):\n",
    "            diff_html += f\"<div style='color:blue;'>{line.rstrip()}</div>\"\n",
    "        else:\n",
    "            diff_html += f\"{line.rstrip()}<br>\"\n",
    "    return diff_html\n",
    "\n",
    "newline_char = '\\n'\n",
    "\n",
    "def rat(question: str, document_path: str):\n",
    "    # Initialize document parser\n",
    "    doc_parser = DocumentParser()\n",
    "    \n",
    "    print(f\"{datetime.now()} [INFO] Parsing document...\")\n",
    "    chunks = doc_parser.parse_and_chunk(document_path)\n",
    "    if not chunks:\n",
    "        return \"Error: Could not parse document\", \"\"\n",
    "    \n",
    "    \n",
    "    print(f\"{datetime.now()} [INFO] Generating draft...\")\n",
    "    draft = get_draft(question)\n",
    "    print(f\"{datetime.now()} [INFO] Return draft.\")\n",
    "    # print(f\"##################### DRAFT #######################\")\n",
    "    # print(draft)\n",
    "    # print(f\"#####################  END  #######################\")\n",
    "\n",
    "    print(f\"{datetime.now()} [INFO] Processing draft ...\")\n",
    "    # draft_paragraphs = split_draft(draft)\n",
    "    draft_paragraphs = split_draft_openai(question, draft)\n",
    "    \n",
    "    \n",
    "    print(f\"{datetime.now()} [INFO] Draft is splitted into {len(draft_paragraphs)} sections.\")\n",
    "    \n",
    "    answer = \"\"\n",
    "    for i, p in enumerate(draft_paragraphs):\n",
    "        # print(str(i)*80)\n",
    "        print(f\"{datetime.now()} [INFO] Revising {i+1}/{len(draft_paragraphs)} sections ...\")\n",
    "        answer = answer + '\\n\\n' + p\n",
    "        # print(f\"[{i}/{len(draft_paragraphs)}] Original Answer:\\n{answer.replace(newline_char, ' ')}\")\n",
    "\n",
    "        # query = get_query(question, answer)\n",
    "        print(f\"{datetime.now()} [INFO] Generating query ...\")\n",
    "        res = run_with_timeout(get_query_wrapper, 30, question, answer)\n",
    "        if not res:\n",
    "            print(f\"{datetime.now()} [INFO] Generating query timeout, skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            query = res\n",
    "        print(f\">>> {i}/{len(draft_paragraphs)} Query: {query.replace(newline_char, ' ')}\")\n",
    "\n",
    "        print(f\"{datetime.now()} [INFO] Crawling network pages ...\")\n",
    "        # content = get_content(query)\n",
    "        # res = run_with_timeout(get_content_wrapper, 30, query)\n",
    "        # if not res:\n",
    "        #     print(f\"{datetime.now()} [INFO] Parsing network pages timeout, skipping ...\")\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     content = res\n",
    "\n",
    "        LIMIT = 2\n",
    "        for j, c in enumerate(draft_paragraphs):\n",
    "            if  j >= LIMIT: # limit rge number of network pages\n",
    "                break\n",
    "            print(f\"{datetime.now()} [INFO] Revising answers with retrieved network pages...[{j}/{min(len(draft_paragraphs),LIMIT)}]\")\n",
    "            # answer = get_revise_answer(question, answer, c)\n",
    "            res = run_with_timeout(get_revise_answer_wrapper, 30, question, answer, c)\n",
    "            if not res:\n",
    "                print(f\"{datetime.now()} [INFO] Revising answers timeout, skipping ...\")\n",
    "                continue\n",
    "            else:\n",
    "                diff_html = generate_diff_html(answer, res)\n",
    "                display(HTML(diff_html))\n",
    "                answer = res\n",
    "            print(f\"{datetime.now()} [INFO] Answer revised [{j}/{min(len(draft_paragraphs),3)}]\")\n",
    "        # print(f\"[{i}/{len(draft_paragraphs)}] REVISED ANSWER:\\n {answer.replace(newline_char, ' ')}\")\n",
    "        # print()\n",
    "    res = run_with_timeout(get_reflect_answer_wrapper, 30, question, answer)\n",
    "    if not res:\n",
    "        print(f\"{datetime.now()} [INFO] Reflecting answers timeout, skipping next steps...\")\n",
    "    else:\n",
    "        answer = res\n",
    "    return draft, answer\n",
    "\n",
    "page_title = \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\"\n",
    "page_md = \"\"\"\n",
    "# RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\n",
    "\n",
    "We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method — retrieval-augmented thoughts (RAT) — revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated.\n",
    "\n",
    "Applying RAT to various base models substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning.\n",
    "\n",
    "Feel free to try our demo!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def clear_func():\n",
    "    return \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "with gr.Blocks(title = page_title) as demo:\n",
    "   \n",
    "    gr.Markdown(page_md)\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document\")\n",
    "\n",
    "    with gr.Row():\n",
    "        chatgpt_box = gr.Textbox(\n",
    "            label = \"ChatGPT\",\n",
    "            placeholder = \"Response from ChatGPT with zero-shot chain-of-thought.\",\n",
    "            elem_id = \"chatgpt\"\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        stream_box = gr.Textbox(\n",
    "            label = \"Streaming\",\n",
    "            placeholder = \"Interactive response with RAT...\",\n",
    "            elem_id = \"stream\",\n",
    "            lines = 10,\n",
    "            visible = False\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        rat_box = gr.Textbox(\n",
    "            label = \"RAT\",\n",
    "            placeholder = \"Final response with RAT ...\",\n",
    "            elem_id = \"rat\",\n",
    "            lines = 6\n",
    "        )\n",
    "\n",
    "    with gr.Column(elem_id=\"instruction_row\"):\n",
    "        with gr.Row():\n",
    "            instruction_box = gr.Textbox(\n",
    "                label = \"instruction\",\n",
    "                placeholder = \"Enter your instruction here\",\n",
    "                lines = 2,\n",
    "                elem_id=\"instruction\",\n",
    "                interactive=True,\n",
    "                visible=True\n",
    "            )\n",
    "        # with gr.Row():\n",
    "        #     model_radio = gr.Radio([\"gpt-3.5-turbo\", \"gpt-4\", \"GPT-4-turbo\"], elem_id=\"model_radio\", value=\"gpt-3.5-turbo\", \n",
    "        #                         label='GPT model', \n",
    "        #                         show_label=True,\n",
    "        #                         interactive=True, \n",
    "        #                         visible=True) \n",
    "        #     openai_api_key_textbox = gr.Textbox(\n",
    "        #         label='OpenAI API key',\n",
    "        #         placeholder=\"Paste your OpenAI API key (sk-...) and hit Enter\", \n",
    "        #         show_label=True, \n",
    "        #         lines=1, \n",
    "        #         type='password')\n",
    "            \n",
    "    # openai_api_key_textbox.change(set_openai_api_key,\n",
    "    #     inputs=[openai_api_key_textbox],\n",
    "    #     outputs=[])\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\n",
    "            value=\"submit\", visible=True, interactive=True\n",
    "        )\n",
    "        clear_btn = gr.Button(\n",
    "            value=\"clear\", visible=True, interactive=True\n",
    "        )\n",
    "        regenerate_btn = gr.Button(\n",
    "            value=\"regenerate\", visible=True, interactive=True\n",
    "        )\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn = rat,\n",
    "        inputs = [instruction_box, file_input],\n",
    "        outputs = [chatgpt_box, rat_box]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn = clear_func,\n",
    "        inputs = [],\n",
    "        outputs = [instruction_box, chatgpt_box, rat_box]\n",
    "    )\n",
    "\n",
    "    regenerate_btn.click(\n",
    "        fn = rat,\n",
    "        inputs = [instruction_box],\n",
    "        outputs = [chatgpt_box, rat_box]\n",
    "    )\n",
    "\n",
    "    examples = gr.Examples(\n",
    "        examples=[\n",
    "            # \"I went to the supermarket yesterday.\", \n",
    "            # \"Helen is a good swimmer.\"\n",
    "            \"Write a survey of retrieval-augmented generation in Large Language Models.\",\n",
    "            \"Introduce Jin-Yong's life and his works.\",\n",
    "            \"Summarize the American Civil War according to the timeline.\",\n",
    "            \"Describe the life and achievements of Marie Curie\"\n",
    "            ],\n",
    "        inputs=[instruction_box]\n",
    "        )\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
