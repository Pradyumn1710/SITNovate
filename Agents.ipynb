{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "# from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from langchain.tools import Tool\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process, Queue\n",
    "from difflib import unified_diff\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = ChatOllama(\n",
    "    model=\"llama 3.1\",\n",
    "    callback_manager=callback_manager,\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_CLOUD_API_KEY=\"llx-nxrF5SQ5MnbMrKhrb1HqfdFA3YajxTDjtjfctsyebDSdBa7W\"\n",
    "\n",
    "# parser = LlamaParse(\n",
    "#     api_key=LLAMA_CLOUD_API_KEY,\n",
    "#     result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    "# )\n",
    "\n",
    "# # use SimpleDirectoryReader to parse our file\n",
    "# file_extractor = {\".pdf\": parser}\n",
    "# documents = SimpleDirectoryReader(input_files=['/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'], file_extractor=file_extractor).load_data()\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://:\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.embed_model = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
    "# vector_store = QdrantVectorStore(client=client, collection_name=\"Hecker\")\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     # embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############SEARCH#####################\n",
    "def get_search(query:str=\"\", k:int=1): # get the top-k resources with google\n",
    "    search = GoogleSearchAPIWrapper(k=k)\n",
    "    def search_results(query):\n",
    "        return search.results(query, k)\n",
    "    tool = Tool(\n",
    "        name=\"Google Search Snippets\",\n",
    "        description=\"Search Google for recent results.\",\n",
    "        func=search_results,\n",
    "    )\n",
    "    ref_text = tool.run(query)\n",
    "    if 'Result' not in ref_text[0].keys():\n",
    "        return ref_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "def get_page_content(link:str):\n",
    "    loader = AsyncHtmlLoader([link])\n",
    "    docs = loader.load()\n",
    "    html2text = Html2TextTransformer()\n",
    "    docs_transformed = html2text.transform_documents(docs)\n",
    "    if len(docs_transformed) > 0:\n",
    "        return docs_transformed[0].page_content\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_content(query):\n",
    "    res = get_search(query, 1)\n",
    "    if not res:\n",
    "        print(\">>> No good Google Search Result was found\")\n",
    "        return None\n",
    "    search_results = res[0]\n",
    "    link = search_results['link'] # title, snippet\n",
    "    res = get_page_content(link)\n",
    "    if not res:\n",
    "        print(f\">>> No content was found in {link}\")\n",
    "        return None\n",
    "    retrieved_text = res\n",
    "    trunked_texts = chunk_texts(retrieved_text, 1500)\n",
    "    trunked_texts = [trunked_text.replace('\\n', \" \") for trunked_text in trunked_texts]\n",
    "    return trunked_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def chunk_text_by_sentence(text, chunk_size=2048):\n",
    "    \"\"\"Chunk the $text into sentences with less than 2k tokens.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunked_text = []\n",
    "    curr_chunk = []\n",
    "    for sentence in sentences:\n",
    "        if num_tokens_from_string(\". \".join(curr_chunk)) + num_tokens_from_string(sentence) + 2 <= chunk_size:\n",
    "            curr_chunk.append(sentence)\n",
    "        else:\n",
    "            chunked_text.append(\". \".join(curr_chunk))\n",
    "            curr_chunk = [sentence]\n",
    "    if curr_chunk:\n",
    "        chunked_text.append(\". \".join(curr_chunk))\n",
    "    return chunked_text[0]\n",
    "\n",
    "def chunk_text_front(text, chunk_size = 2048):\n",
    "    '''\n",
    "    get the first `trunk_size` token of text\n",
    "    '''\n",
    "    chunked_text = \"\"\n",
    "    tokens = num_tokens_from_string(text)\n",
    "    if tokens < chunk_size:\n",
    "        return text\n",
    "    else:\n",
    "        ratio = float(chunk_size) / tokens\n",
    "        char_num = int(len(text) * ratio)\n",
    "        return text[:char_num]\n",
    "\n",
    "def chunk_texts(text, chunk_size = 2048):\n",
    "    '''\n",
    "    trunk the text into n parts, return a list of text\n",
    "    [text, text, text]\n",
    "    '''\n",
    "    tokens = num_tokens_from_string(text)\n",
    "    if tokens < chunk_size:\n",
    "        return [text]\n",
    "    else:\n",
    "        texts = []\n",
    "        n = int(tokens/chunk_size) + 1\n",
    "        \n",
    "        part_length = len(text) // n\n",
    "        \n",
    "        extra = len(text) % n\n",
    "        parts = []\n",
    "        start = 0\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            end = start + part_length + (1 if i < extra else 0)\n",
    "            parts.append(text[start:end])\n",
    "            start = end\n",
    "        return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def get_draft(question):\n",
    "    # Getting the draft answer\n",
    "    draft_prompt = '''\n",
    "IMPORTANT:\n",
    "Try to answer this question/instruction with step-by-step thoughts and make the answer more structural.\n",
    "Use `\\n\\n` to split the answer into several paragraphs.\n",
    "Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "    # openai_client = OpenAI(api_key=openai.api_key)\n",
    "    draft_agent = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    response = draft_agent.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"{question}\\n{draft_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import tiktoken\n",
    "from typing import List, Optional\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "class DocumentParser:\n",
    "    def __init__(self, result_type=\"markdown\",collection_name=\"Hecker\"):\n",
    "        self.parser = LlamaParse(result_type=result_type)\n",
    "        self.file_extractor = {\".pdf\": self.parser}\n",
    "        self.Settings.embed_model = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
    "        self.client = qdrant_client.QdrantClient(host=\"localhost\", port=6333)\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def parse_document(self, file_path):\n",
    "        \"\"\"Parse document using LlamaParse\"\"\"\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_files=[file_path], \n",
    "            file_extractor=self.file_extractor\n",
    "        ).load_data()\n",
    "        return [doc.text for doc in documents]\n",
    "    \n",
    "    def chunk_and_embed(self, texts, chunk_size=1500):\n",
    "        \"\"\"Chunk texts and create embeddings\"\"\"\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            # Split text into smaller chunks\n",
    "            sentences = text.split('. ')\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if current_length + len(sentence) < chunk_size:\n",
    "                    current_chunk.append(sentence)\n",
    "                    current_length += len(sentence)\n",
    "                else:\n",
    "                    chunks.append('. '.join(current_chunk))\n",
    "                    current_chunk = [sentence]\n",
    "                    current_length = len(sentence)\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append('. '.join(current_chunk))\n",
    "        \n",
    "        # Create embeddings\n",
    "        embeddings = self.embeddings.embed_documents(chunks)\n",
    "        return chunks, embeddings\n",
    "    \n",
    "    def index_document(self, file_path):\n",
    "        \"\"\"Process and index document\"\"\"\n",
    "        # Parse document\n",
    "        texts = self.parse_document(file_path)\n",
    "        \n",
    "        # Create chunks and embeddings\n",
    "        chunks, embeddings = self.chunk_and_embed(texts)\n",
    "        \n",
    "        # Store in Qdrant\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=[\n",
    "                {\"id\": idx, \"vector\": emb, \"payload\": {\"text\": chunk}}\n",
    "                for idx, (chunk, emb) in enumerate(zip(chunks, embeddings))\n",
    "            ]\n",
    "        )\n",
    "        return chunks\n",
    "    \n",
    "    def semantic_search(self, query, top_k=2):\n",
    "        \"\"\"Search for relevant chunks\"\"\"\n",
    "        query_vector = self.embeddings.embed_query(query)\n",
    "        \n",
    "        results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k\n",
    "        )\n",
    "        \n",
    "        return [hit.payload[\"text\"] for hit in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_draft(draft, split_char = '\\n\\n'):\n",
    "    paragraphs = draft.split(split_char)\n",
    "    draft_paragraphs = [para for para in paragraphs if len(para)>5]\n",
    "    # print(f\"The draft answer has {len(draft_paragraphs)}\")\n",
    "    return draft_paragraphs\n",
    "\n",
    "def split_draft_openai(question, answer, NUM_PARAGRAPHS = 4):\n",
    "    split_prompt = f'''\n",
    "Split the answer of the question into multiple paragraphs with each paragraph containing a complete thought.\n",
    "The answer should be splited into less than {NUM_PARAGRAPHS} paragraphs.\n",
    "Use ## as splitting char to seperate the paragraphs.\n",
    "So you should output the answer with ## to split the paragraphs.\n",
    "**IMPORTANT**\n",
    "Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    splited_answer = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Question: {question}\\n\\n##Response: {answer}\\n\\n##Instruction: {split_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    split_draft_paragraphs = split_draft(splited_answer, split_char = '##')\n",
    "    return split_draft_paragraphs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(question, answer):\n",
    "    query_prompt = '''\n",
    "I want to verify the content correctness of the given question, especially the last sentences.\n",
    "Please summarize the content with the corresponding question.\n",
    "This summarization will be used as a query to search with Bing search engine.\n",
    "The query should be short but need to be specific to promise Bing can find related knowledge or pages.\n",
    "You can also use search syntax to make the query short and clear enough for the search engine to find relevant language data.\n",
    "Try to make the query as relevant as possible to the last few sentences in the content.\n",
    "**IMPORTANT**\n",
    "Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "    # openai_client = OpenAI(api_key = openai.api_key)\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    query = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Question: {question}\\n\\n##Response: {answer}\\n\\n##Instruction: {query_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    return query.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revise_answer(question, answer, content):\n",
    "    revise_prompt = '''\n",
    "I want to revise the answer according to retrieved related text of the question in WIKI pages.\n",
    "You need to check whether the answer is correct.\n",
    "If you find some errors in the answer, revise the answer to make it better.\n",
    "If you find some necessary details are ignored, add it to make the answer more plausible according to the related text.\n",
    "If you find the answer is right and do not need to add more details, just output the original answer directly.\n",
    "**IMPORTANT**\n",
    "Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.\n",
    "Add more details from retrieved text to the answer.\n",
    "Split the paragraphs with \\n\\n characters.\n",
    "Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.\n",
    "'''\n",
    "    # openai_client = OpenAI(api_key = openai.api_key)\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    revised_answer = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Existing Text in Wiki Web: {content}\\n\\n##Question: {question}\\n\\n##Answer: {answer}\\n\\n##Instruction: {revise_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    return revised_answer.content\n",
    "\n",
    "def get_reflect_answer(question, answer):\n",
    "    reflect_prompt = '''\n",
    "Give a title for the answer of the question.\n",
    "And add a subtitle to each paragraph in the answer and output the final answer using markdown format. \n",
    "This will make the answer to this question look more structured for better understanding.\n",
    "**IMPORTANT**\n",
    "Try to keep the structure (multiple paragraphs with its subtitles) in the response and make it more structual for understanding.\n",
    "Split the paragraphs with \\n\\n characters.\n",
    "Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.\n",
    "'''\n",
    "\n",
    "    Ollama_client = ChatOllama(model=local_llm, callback_manager=callback_manager, format=\"json\", temperature=1)\n",
    "    reflected_answer = Ollama_client.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=f\"##Question:\\n{question}\\n\\n##Answer:\\n{answer}\\n\\n##Instruction:\\n{reflect_prompt}\")\n",
    "        ]\n",
    "    })\n",
    "    return reflected_answer.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_wrapper(q, question, answer):\n",
    "    result = get_query(question, answer)\n",
    "    q.put(result)\n",
    "\n",
    "# def get_content_wrapper(q, query):\n",
    "#     result = get_content(query)\n",
    "#     q.put(result)\n",
    "\n",
    "def get_revise_answer_wrapper(q, question, answer, content):\n",
    "    result = get_revise_answer(question, answer, content)\n",
    "    q.put(result)\n",
    "\n",
    "def get_reflect_answer_wrapper(q, question, answer):\n",
    "    result = get_reflect_answer(question, answer)\n",
    "    q.put(result)\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "def run_with_timeout(func, timeout, *args, **kwargs):\n",
    "    q = Queue()  \n",
    "    p = Process(target=func, args=(q, *args), kwargs=kwargs)\n",
    "    p.start()\n",
    "    p.join(timeout)\n",
    "    if p.is_alive():\n",
    "        print(f\"{datetime.now()} [INFO] Function {str(func)} running timeout ({timeout}s), terminating...\")\n",
    "        p.terminate()\n",
    "        p.join() \n",
    "        result = None\n",
    "    else:\n",
    "        print(f\"{datetime.now()} [INFO] Function {str(func)} executed successfully.\")\n",
    "        result = q.get()  # 从队列中获取结果\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from difflib import unified_diff\n",
    "from IPython.display import display, HTML\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_diff_html(text1, text2):\n",
    "    diff = unified_diff(text1.splitlines(keepends=True),\n",
    "                        text2.splitlines(keepends=True),\n",
    "                        fromfile='text1', tofile='text2')\n",
    "\n",
    "    diff_html = \"\"\n",
    "    for line in diff:\n",
    "        if line.startswith('+'):\n",
    "            diff_html += f\"<div style='color:green;'>{line.rstrip()}</div>\"\n",
    "        elif line.startswith('-'):\n",
    "            diff_html += f\"<div style='color:red;'>{line.rstrip()}</div>\"\n",
    "        elif line.startswith('@'):\n",
    "            diff_html += f\"<div style='color:blue;'>{line.rstrip()}</div>\"\n",
    "        else:\n",
    "            diff_html += f\"{line.rstrip()}<br>\"\n",
    "    return diff_html\n",
    "\n",
    "newline_char = '\\n'\n",
    "\n",
    "def rat(question):\n",
    "    print(f\"{datetime.now()} [INFO] Generating draft...\")\n",
    "    draft = get_draft(question)\n",
    "    print(f\"{datetime.now()} [INFO] Return draft.\")\n",
    "    # print(f\"##################### DRAFT #######################\")\n",
    "    # print(draft)\n",
    "    # print(f\"#####################  END  #######################\")\n",
    "\n",
    "    print(f\"{datetime.now()} [INFO] Processing draft ...\")\n",
    "    # draft_paragraphs = split_draft(draft)\n",
    "    draft_paragraphs = split_draft_openai(question, draft)\n",
    "    print(f\"{datetime.now()} [INFO] Draft is splitted into {len(draft_paragraphs)} sections.\")\n",
    "    answer = \"\"\n",
    "    for i, p in enumerate(draft_paragraphs):\n",
    "        # print(str(i)*80)\n",
    "        print(f\"{datetime.now()} [INFO] Revising {i+1}/{len(draft_paragraphs)} sections ...\")\n",
    "        answer = answer + '\\n\\n' + p\n",
    "        # print(f\"[{i}/{len(draft_paragraphs)}] Original Answer:\\n{answer.replace(newline_char, ' ')}\")\n",
    "\n",
    "        # query = get_query(question, answer)\n",
    "        print(f\"{datetime.now()} [INFO] Generating query ...\")\n",
    "        res = run_with_timeout(get_query_wrapper, 30, question, answer)\n",
    "        if not res:\n",
    "            print(f\"{datetime.now()} [INFO] Generating query timeout, skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            query = res\n",
    "        print(f\">>> {i}/{len(draft_paragraphs)} Query: {query.replace(newline_char, ' ')}\")\n",
    "\n",
    "        print(f\"{datetime.now()} [INFO] Crawling network pages ...\")\n",
    "        # content = get_content(query)\n",
    "        res = run_with_timeout(get_content_wrapper, 30, query)\n",
    "        if not res:\n",
    "            print(f\"{datetime.now()} [INFO] Parsing network pages timeout, skipping ...\")\n",
    "            continue\n",
    "        else:\n",
    "            content = res\n",
    "\n",
    "        LIMIT = 2\n",
    "        for j, c in enumerate(content):\n",
    "            if  j >= LIMIT: # limit rge number of network pages\n",
    "                break\n",
    "            print(f\"{datetime.now()} [INFO] Revising answers with retrieved network pages...[{j}/{min(len(content),LIMIT)}]\")\n",
    "            # answer = get_revise_answer(question, answer, c)\n",
    "            res = run_with_timeout(get_revise_answer_wrapper, 30, question, answer, c)\n",
    "            if not res:\n",
    "                print(f\"{datetime.now()} [INFO] Revising answers timeout, skipping ...\")\n",
    "                continue\n",
    "            else:\n",
    "                diff_html = generate_diff_html(answer, res)\n",
    "                display(HTML(diff_html))\n",
    "                answer = res\n",
    "            print(f\"{datetime.now()} [INFO] Answer revised [{j}/{min(len(content),3)}]\")\n",
    "        # print(f\"[{i}/{len(draft_paragraphs)}] REVISED ANSWER:\\n {answer.replace(newline_char, ' ')}\")\n",
    "        # print()\n",
    "    res = run_with_timeout(get_reflect_answer_wrapper, 30, question, answer)\n",
    "    if not res:\n",
    "        print(f\"{datetime.now()} [INFO] Reflecting answers timeout, skipping next steps...\")\n",
    "    else:\n",
    "        answer = res\n",
    "    return draft, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/utils.py:1017: UserWarning: Expected 1 arguments for function <function rat at 0x77268f16cae0>, received 2.\n",
      "  warnings.warn(\n",
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/utils.py:1025: UserWarning: Expected maximum 1 arguments for function <function rat at 0x77268f16cae0>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/blocks.py\", line 2098, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/blocks.py\", line 1645, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/gradio/utils.py\", line 883, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: rat() takes 1 positional argument but 2 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_title = \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\"\n",
    "page_md = \"\"\"\n",
    "# RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\n",
    "\n",
    "We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method — retrieval-augmented thoughts (RAT) — revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated.\n",
    "\n",
    "Applying RAT to various base models substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning.\n",
    "\n",
    "Feel free to try our demo!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def clear_func():\n",
    "    return \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "with gr.Blocks(title = page_title) as demo:\n",
    "   \n",
    "    gr.Markdown(page_md)\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document\")\n",
    "\n",
    "    with gr.Row():\n",
    "        chatgpt_box = gr.Textbox(\n",
    "            label = \"ChatGPT\",\n",
    "            placeholder = \"Response from ChatGPT with zero-shot chain-of-thought.\",\n",
    "            elem_id = \"chatgpt\"\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        stream_box = gr.Textbox(\n",
    "            label = \"Streaming\",\n",
    "            placeholder = \"Interactive response with RAT...\",\n",
    "            elem_id = \"stream\",\n",
    "            lines = 10,\n",
    "            visible = False\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        rat_box = gr.Textbox(\n",
    "            label = \"RAT\",\n",
    "            placeholder = \"Final response with RAT ...\",\n",
    "            elem_id = \"rat\",\n",
    "            lines = 6\n",
    "        )\n",
    "\n",
    "    with gr.Column(elem_id=\"instruction_row\"):\n",
    "        with gr.Row():\n",
    "            instruction_box = gr.Textbox(\n",
    "                label = \"instruction\",\n",
    "                placeholder = \"Enter your instruction here\",\n",
    "                lines = 2,\n",
    "                elem_id=\"instruction\",\n",
    "                interactive=True,\n",
    "                visible=True\n",
    "            )\n",
    "        # with gr.Row():\n",
    "        #     model_radio = gr.Radio([\"gpt-3.5-turbo\", \"gpt-4\", \"GPT-4-turbo\"], elem_id=\"model_radio\", value=\"gpt-3.5-turbo\", \n",
    "        #                         label='GPT model', \n",
    "        #                         show_label=True,\n",
    "        #                         interactive=True, \n",
    "        #                         visible=True) \n",
    "        #     openai_api_key_textbox = gr.Textbox(\n",
    "        #         label='OpenAI API key',\n",
    "        #         placeholder=\"Paste your OpenAI API key (sk-...) and hit Enter\", \n",
    "        #         show_label=True, \n",
    "        #         lines=1, \n",
    "        #         type='password')\n",
    "            \n",
    "    # openai_api_key_textbox.change(set_openai_api_key,\n",
    "    #     inputs=[openai_api_key_textbox],\n",
    "    #     outputs=[])\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\n",
    "            value=\"submit\", visible=True, interactive=True\n",
    "        )\n",
    "        clear_btn = gr.Button(\n",
    "            value=\"clear\", visible=True, interactive=True\n",
    "        )\n",
    "        regenerate_btn = gr.Button(\n",
    "            value=\"regenerate\", visible=True, interactive=True\n",
    "        )\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn = rat,\n",
    "        inputs = [instruction_box, file_input],\n",
    "        outputs = [chatgpt_box, rat_box]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn = clear_func,\n",
    "        inputs = [],\n",
    "        outputs = [instruction_box, chatgpt_box, rat_box]\n",
    "    )\n",
    "\n",
    "    regenerate_btn.click(\n",
    "        fn = rat,\n",
    "        inputs = [instruction_box],\n",
    "        outputs = [chatgpt_box, rat_box]\n",
    "    )\n",
    "\n",
    "    examples = gr.Examples(\n",
    "        examples=[\n",
    "            # \"I went to the supermarket yesterday.\", \n",
    "            # \"Helen is a good swimmer.\"\n",
    "            \"Write a survey of retrieval-augmented generation in Large Language Models.\",\n",
    "            \"Introduce Jin-Yong's life and his works.\",\n",
    "            \"Summarize the American Civil War according to the timeline.\",\n",
    "            \"Describe the life and achievements of Marie Curie\"\n",
    "            ],\n",
    "        inputs=[instruction_box]\n",
    "        )\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
