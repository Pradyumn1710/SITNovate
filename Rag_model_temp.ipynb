{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !capture --no-stderr\n",
    "# !pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"nomic[local]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"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\")\n",
    "_set_env(\"nk-p4RbLXYiBQInfAQyrlCssat_n9w-697uXrq4dlCmq0o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import Document as LlamaDocument\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_cloud_services import LlamaParse\n",
    "# from llama_index.core import Document as LlamaDocument\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from dataclasses import dataclass\n",
    "# from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "\n",
    "# LLAMA_CLOUD_API_KEY=\"llx-nxrF5SQ5MnbMrKhrb1HqfdFA3YajxTDjtjfctsyebDSdBa7W\"\n",
    "\n",
    "# @dataclass\n",
    "# class Document:\n",
    "#     \"\"\"Document class compatible with LangChain\"\"\"\n",
    "#     page_content: str\n",
    "#     metadata: dict = None\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         if self.metadata is None:\n",
    "#             self.metadata = {}\n",
    "\n",
    "# def load_and_process_documents(file_paths):\n",
    "#     \"\"\"Load and process PDF documents using LlamaParse\"\"\"\n",
    "#     try:\n",
    "#         # Configure parser\n",
    "#         parser = LlamaParse(\n",
    "#         result_type=\"markdown\",\n",
    "#         api_key=LLAMA_CLOUD_API_KEY\n",
    "#         )        \n",
    "#         file_extractor = {\".pdf\": parser}\n",
    "        \n",
    "#         # Load raw documents\n",
    "#         raw_docs = SimpleDirectoryReader(\n",
    "#             input_files=file_paths,\n",
    "#             file_extractor=file_extractor\n",
    "#         ).load_data()\n",
    "        \n",
    "#         # Convert to LangChain documents\n",
    "#         documents = [\n",
    "#             Document(\n",
    "#                 page_content=doc.text,\n",
    "#                 metadata={\"source\": file_paths}\n",
    "#             ) for doc in raw_docs\n",
    "#         ]\n",
    "        \n",
    "#         return documents\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing documents: {str(e)}\")\n",
    "#         raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA_CLOUD_API_KEY=\"llx-nxrF5SQ5MnbMrKhrb1HqfdFA3YajxTDjtjfctsyebDSdBa7W\"\n",
    "\n",
    "# def process_pdf(file_path):\n",
    "#     try:\n",
    "#         # Configure LlamaParse\n",
    "#         parser = LlamaParse(result_type=\"markdown\",api_key=LLAMA_CLOUD_API_KEY)\n",
    "#         file_extractor = {\".pdf\": parser}\n",
    "        \n",
    "#         # Load document\n",
    "#         raw_docs = SimpleDirectoryReader(\n",
    "#             input_files=[file_path],\n",
    "#             file_extractor=file_extractor\n",
    "#         ).load_data()\n",
    "        \n",
    "#         # Convert to LangChain format\n",
    "#         documents = [\n",
    "#             Document(\n",
    "#                 page_content=doc.text,\n",
    "#                 metadata={\"source\": file_path}\n",
    "#             ) for doc in raw_docs\n",
    "#         ]\n",
    "        \n",
    "#         # Split text\n",
    "#         text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#             chunk_size=1000,\n",
    "#             chunk_overlap=5\n",
    "#         )\n",
    "#         doc_splits = text_splitter.split_documents(documents)\n",
    "        \n",
    "#         # Store in VectorDB\n",
    "#         vectorstore = Chroma.from_documents(\n",
    "#             documents=doc_splits,\n",
    "#             collection_name=\"rag-chroma\",\n",
    "#             embedding=NomicEmbeddings(\n",
    "#                 model=\"nomic-embed-text-v1.5\",\n",
    "#                 inference_mode=\"local\"\n",
    "#             ),\n",
    "#         )\n",
    "#         return vectorstore\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing PDF: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "# from llama_index.core import Settings\n",
    "# import qdrant_client\n",
    "# from IPython.display import Markdown, display\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "# from llama_index.core import StorageContext\n",
    "# from langchain_nomic.embeddings import NomicEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_files = [\"/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf\"]\n",
    "# retriever = process_pdf(local_files).as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id f4365c32-46c8-4685-bd18-86f7acfc041f\n",
      "Generated 11 chunks\n"
     ]
    }
   ],
   "source": [
    "# local_files = [\"/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf\"]\n",
    "# documents = load_and_process_documents(local_files)\n",
    "# print(f\"Generated {len(documents)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define the paths to your local files\n",
    "local_files = [\n",
    "    \"/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf\",\n",
    "]\n",
    "\n",
    "# Class to wrap the file content\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Function to load the content from local PDF files\n",
    "def load_local_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    content = \"\"\n",
    "    for page in doc:\n",
    "        content += page.get_text()\n",
    "    return Document(content, metadata={\"source\": file_path})\n",
    "\n",
    "# Load the documents from the local files\n",
    "docs = [load_local_pdf(file_path) for file_path in local_files]\n",
    "\n",
    "# Split the documents using the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='Citation: Yaseen; Kwon, O.-J.; Kim, J.;\\nJamil, S.; Lee, J.; Ullah, F. Next-Gen\\nDynamic Hand Gesture Recognition:\\nMediaPipe, Inception-v3 and\\nLSTM-Based Enhanced Deep\\nLearning Model. Electronics 2024, 13,\\n3233. https://doi.org/10.3390/\\nelectronics13163233\\nAcademic Editors: Fath U Min Ullah,\\nEstefanía Talavera and Nuno\\nGonçalves\\nReceived: 12 July 2024\\nRevised: 6 August 2024\\nAccepted: 12 August 2024\\nPublished: 15 August 2024\\nCopyright: © 2024 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed\\nunder\\nthe\\nterms\\nand\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nelectronics\\nArticle\\nNext-Gen Dynamic Hand Gesture Recognition: MediaPipe,\\nInception-v3 and LSTM-Based Enhanced Deep Learning Model\\nYaseen 1\\n, Oh-Jin Kwon 1,*\\n, Jaeho Kim 2\\n, Sonain Jamil 3\\n, Jinhee Lee 1 and Faiz Ullah 1\\n1\\nDepartment of Electronics Engineering, Sejong University, Seoul 05006, Republic of Korea;\\nyaseen@sju.ac.kr (Y.); jinee5025@sju.ac.kr (J.L.); faiz@sju.ac.kr (F.U.)\\n2\\nDepartment of Electrical Engineering, Sejong University, Seoul 05006, Republic of Korea; kimjh@sejong.ac.kr\\n3\\nDepartment of Computer Science, Norwegian University of Science and Technology (NTNU),\\n2815 Gjovik, Norway; sonainj@stud.ntnu.no\\n*\\nCorrespondence: ojkwon@sejong.ac.kr\\nAbstract: Gesture recognition is crucial in computer vision-based applications, such as drone control,\\ngaming, virtual and augmented reality (VR/AR), and security, especially in human–computer\\ninteraction (HCI)-based systems. There are two types of gesture recognition systems, i.e., static and\\ndynamic. However, our focus in this paper is on dynamic gesture recognition. In dynamic hand\\ngesture recognition systems, the sequences of frames, i.e., temporal data, pose significant processing\\nchallenges and reduce efficiency compared to static gestures. These data become multi-dimensional\\ncompared to static images because spatial and temporal data are being processed, which demands\\ncomplex deep learning (DL) models with increased computational costs. This article presents a\\nnovel triple-layer algorithm that efficiently reduces the 3D feature map into 1D row vectors and\\nenhances the overall performance. First, we process the individual images in a given sequence using\\nthe MediaPipe framework and extract the regions of interest (ROI). The processed cropped image\\nis then passed to the Inception-v3 for the 2D feature extractor. Finally, a long short-term memory\\n(LSTM) network is used as a temporal feature extractor and classifier. Our proposed method achieves\\nan average accuracy of more than 89.7%. The experimental results also show that the proposed\\nframework outperforms existing state-of-the-art methods.\\nKeywords: dynamic hand gesture recognition; hybrid deep learning; dimensionality reduction;\\ntemporal data classification; transfer learning; vision-based drone control\\n1. Introduction\\nHand gesture recognition (HGR) is a popular research area due to its scope in applica-\\ntions like human–computer interaction (HCI), drone control, virtual/augmented reality,\\nsign language interpretation, and other interaction-based systems [1,2]. Gestures are natu-\\nral and intuitive to communicate, sometimes with barely apparent body movements [3].\\nComputer vision exploits these gestures, making it possible to interact with technology\\nwithout aided sensors. These gestures can be of two types: static gestures and dynamic\\ngestures. Static gestures represent a still hand gesture or hand posture in a single image\\nframe, i.e., relying fundamentally on spatial information [4]. Dynamic hand gestures are\\nrepresented by continuous hand motion and are therefore represented by sequences of\\nimages, i.e., by temporal information [5]. Compared to static gestures, dynamic gestures\\ncan represent and enable a wider range of activities [6]. Vision-based hand gestures provide'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='a touchless interface, and the gestures are recorded by an RGB camera device [7].\\nDeep learning algorithms, like convolutional neural networks (CNN), are used for\\nhand gesture recognition. This method recognizes hand gestures with high accuracy\\nwithout using any aided sensors [8,9]. Thus, the user can interact with a computer system\\nmore naturally.\\nElectronics 2024, 13, 3233. https://doi.org/10.3390/electronics13163233\\nhttps://www.mdpi.com/journal/electronics\\nElectronics 2024, 13, 3233\\n2 of 11\\nResearchers face several challenges, and they have been addressed quite well. How-\\never, one common challenge related to dynamic hand gestures is the variability in the length\\nof sequences of image frames in a gesture clip. Variable-length gesture clips pose challenges\\nin data processing, model training, and inference. This also increases the computational\\ntime and power. While work has been conducted on achieving high accuracy, less attention\\nhas been given to handling the complex shape of gestures. Several authors have reported\\ntheir work relating to dynamic hand gestures. Recently, Karsh et al. [4] reported their work\\non the same topic. However, they have tried to focus more on improving the deep learning\\narchitecture first rather than on data processing. The authors in [10] have reported work on\\nskeletal-based hand data. For more realistic environments and practical scenarios, datasets\\nneed to be more diverse, incorporating images taken in a natural environment. Authors\\nin [3] have also contributed to improving the accuracy by proposing a hybrid CNN model.\\nTheir hybrid model consists of a feature extractor 3D CNN module and a classifier module\\nmaking use of the computational cost and power.\\nOverall, the work conducted so far on dynamic hand gestures is significant, but little\\nfocus has been given to the complexity of their temporal aspects. Considering the research\\ngap, we list our contributions as follows:\\n•\\nExtending the work conducted by authors [3], we reduce the dimensionality of the 3D\\nfeature map to 1D while retaining the temporal aspects of the data.\\n•\\nTemporal data that is present in the sequence of frames are utilized efficiently by the\\nproposed hybrid model at a lower computational cost than existing methods.\\n•\\nA lightweight model is proposed, reducing the computation complexity.\\n•\\nThe performance is improved compared to the baseline algorithm. [3]\\nIn summary, we address the temporal complexity of dynamic hand gestures, paving\\nthe path for a vision-based interaction system for dynamic hand gesture recognition. The\\nrest of the paper is described as follows: In Section 2, we discuss related work and methods;\\nSection 3 presents our proposed method; Section 4 evaluates the model and presents the\\nresults. Section 5 includes our discussion, and finally, in the Section 6 conclusions, we point\\nout the limitations, other applications for our method, and future works.\\n2. Related Work\\nIn deep learning (DL), CNNs are used simultaneously for feature extraction and\\nclassification. Using convolution and pooling optimizes feature extraction and eases the\\nclassification process [11]. CNNs were first used in 1990 to recognize handwritten digits.\\nThe foundation of modern CNN architecture was laid down by authors [12]. Based on their\\narchitecture, more robust classification algorithms were proposed, including VGGNet [13],\\nResnet50 [14], and GoogleNet [15]. With the increased depth of these CNN networks, their\\naccuracy improved, and so did their demand for high computational power. Currently,\\nresearchers are trying to build lightweight CNN models for use in devices with low\\ncomputing power and in real-world scenarios [16].\\n2.1. Dynamic Hand Gesture Classification\\nThere are various deep learning architectures for temporal data classification. Some of\\nthem are worth mentioning [17], such as 2D CNNs, which are computationally less expen-\\nsive and are widely used for spatial data feature extraction and classification. The more\\ncomplex model architectures are 3D CNNs; the third dimension comes from temporal\\ndata in these models. Then, two-stream CNNs use 2D CNNs for sparse data classification\\nand recurrent neural networks (RNNs) for temporal data classification. While there are\\nseveral others, the hybrid approaches are the most popular as they increase accuracy while\\nreducing the computational cost. In hybrid models, CNNs are combined with RNN, gated\\nrecurrent neural networks, or long short-term memory (LSTM) [18]. In the domain of hand\\ngesture recognition, there exist numerous state-of-the-art methods [19–21]. Dynamic hand'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='gestures are more complex to process as the temporal data have to be considered, as well as\\nElectronics 2024, 13, 3233\\n3 of 11\\nthe optical flow of the data [22]. While dynamic hand gesture recognition authors propose\\nseveral novel methods, we will discuss the most recent hybrid methods.\\nAuthors in [23] investigated HGR with leap motion sensor-based data, and although\\nthey achieved an accuracy of 97%, the work does not provide a fully touchless interface as in\\nthe case of RGB data. This work was further extended by Huu [24] while using two-stream\\napproaches, achieving a similar accuracy [23]. In the case of 3D CNNs, authors in the\\nwork [25] claimed an accuracy of 90%, but their system is limited to working only under the\\ncondition that a person has to sit close to the camera while performing gestures. Another\\nlimitation is that they use more complex CNN models for their work. Similarly, in the\\ncase of hybrid approaches, another hybrid model was recently proposed by authors [3].\\nThe authors used Google’s Inception-v3 architecture as a feature extractor and LSTM for\\ntemporal data feature extraction and classification. They have achieved an overall accuracy\\nof 84%. However, their work could not process variable-length sequences of dynamic\\nhand gestures, and there is more room for improvement in the accuracy [3]. Even if the\\naccuracy was much higher, the practical use of this model in real-world scenarios is limited,\\nas during live feed from the camera, the length of a gesture clip is always variable [26],\\nwhich the authors couldn’t address in their work [3].\\n2.2. Classification with Hybrid Deep Learning\\nAs we know, for a dynamic hand gesture recognition system, spatial data processing is\\nnot sufficient; temporal relations in sequences of frames have to be exploited [27]. Various\\nstate-of-the-art methods have been reported to work on video classification tasks, espe-\\ncially on dynamic hand gestures using hybrid approaches, i.e., combining CNN or RNN,\\nCNN, and LSTM, and similar other approaches [1]. Various studies have tried exploiting\\nhybrid deep learning models based on CNNs and RNNs for HGR [24,28] and other video\\nclassification tasks [29].\\nIn traditional neural networks, neurons are only connected in a feed-forward direction,\\nwhile in an RNN, the neurons are not only connected in a mixed fashion within a layer but\\nare also connected in opposite directions (both forward and backward) [30]. Furthermore,\\nLSTMs are modified RNNs for the classification of temporal data so that they can store\\nthe data in memory cells. These cells contain the output of the previous state relative\\nto the current input. Through memory cells, the temporal relation between the data can\\nbe retained [31].\\nFigure 1 shows this hybrid approach; CNN is used as a feature extractor as they\\nare suitable for recognizing image patterns. At this stage, the spatial information from\\nindividual images is only processed using 2D CNN, which does not classify but obtains the\\nhidden patterns. The feature maps for all the frames within the sequence are then passed\\ninto the LSTM network. The LSTM extracts features and hidden patterns, exploring tempo-\\nral relations from the feature maps and performing classification. This yields improved\\nclassification and better results [32].\\nFigure 1. A basic CNN-RNN-based hybrid architecture.\\n3. Methodology\\nCompared to static hand gestures, dynamic hand gestures have sequences of frames\\nwhere the hand moment and position within each frame are recorded concerning the time.\\nFor this reason, the hybrid deep neural network architecture that comprises CNN and\\nRNN is well suited for temporal data classification tasks [33]. Our proposed architecture\\nElectronics 2024, 13, 3233\\n4 of 11\\nis inspired by Hax et al. [3]. They have used the Google Inception v3 [34] model for\\nfeature extraction and an LSTM for classification. In our case, we have replaced the Google\\nInception v3 [34] with MediaPipe 0.10.14 in the input layer [35]—to remove unwanted\\nfeatures from the frame and locate the hand gesture efficiently. MediaPipe has a built-in\\nhand landmark model, which returns the knuckle points of the hands in an image with\\nlow computational power. Using this advantage, we locate the region of the hand in an\\nimage and crop the area called the region of interest (ROI). This cropped image is used in\\nthe input for the Inception-v3 layer of feature extraction. The highly efficient approach'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='given number of test instances (1).\\n•\\nRecall: Also called true positive rate as it measures the model’s ability regarding true\\npositives (2).\\n•\\nF1-score: Reflects the model’s ability concerning false positives and negatives (3).\\n•\\nSpecificity: Reflects the model’s ability to correctly identify true negative instances (4).\\nFormulas for each of these metrics are given below:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN\\n(1)\\nRecall =\\nTP\\nTP + FN\\n(2)\\nF1-score = 2 × Precision × Recall\\nPrecision + Recall\\n(3)\\nSpecificity =\\nTN\\nTN + FP\\n(4)\\nElectronics 2024, 13, 3233\\n6 of 11\\nResults based on the above metrics are shown in Table 1. For more visually appealing\\nresults and deeper insights, we have used a confusion matrix as shown in Figures 3 and 4.\\nAlso, to further evaluate our results for the proposed architecture, we replace the\\nLSTM classifier with a gated recurrent unit (GRU) as these are also the most promising\\nmodels [18]. We consider the same data split for this architecture as well. We have also\\nused the Resnet50, replacing Inception v3 in our architecture, for further comparison.\\nFigure 3. Confusion matrix based on validation data.\\nFigure 4. Confusion matrix based on test data.\\nElectronics 2024, 13, 3233\\n7 of 11\\nTable 1. Average results based on the performance metrics for test and validation data.\\nData\\nRecall\\nF1-Score\\nSpecificity\\nAccuracy\\nValidation\\n89.9%\\n89.7%\\n98.0%\\n90.1%\\nTest\\n91.5%\\n90.0%\\n97.9%\\n89.7%\\n4. Results\\nThis Section presents the experimental setup, performance metrics, and visualization\\n(the reason and rationale behind our model architecture and dataset pre-processing).\\n4.1. Experimental Setup\\nThe models were trained for 300 epochs, with early stopping enabled to avoid over-\\nfitting, and the initial learning rate was set to 0.001. The categorical cross-entropy cost\\nfunction was used for training and validation losses for maximum learning. The adam-\\noptimizer was used to iteratively update and adjust the network weights in order to\\nminimize these losses. In the case of activation functions, the tangent hyperbolic function\\nwas used for the LSTM layer and the softmax function for the final output layer. For the\\nintermediate layer (Dense layer), the rectified linear unit (RelU) was employed (see Figure 2).\\nThe machine we used for our pipeline has the following specs: We used an HP desktop\\ncomputer, manufactured by HP Inc., Palo Alto, CA, USA, model Pavilion Gaming Desktop\\nTG01-1xxx, with a RAM of 32 GB, GPU of NVIDIA GeForce RTX 2060 SUPER, and eight\\ncores with clock speed of 2.9 GHz.\\n4.2. Performance Metrics and Visualizations\\nThe proposed hybrid deep learning-based model for hand gesture recognition demon-\\nstrated good performance when evaluated via various metrics, as shown in Table 1. The em-\\npirical tests, both on validation and test results, showcase satisfactory results; an average\\naccuracy of 89.7% on test data and 90.1% on validation data was reported. Other metrics\\nwere also used to assess the model’s ability; we achieved a recall of 91%, an F1-score of\\n90.0%, and a specificity of 98.0%.\\nThe results were also compared to that of other models [3,37–39], as shown in Table 2.\\nThe average accuracy on the test, as well as the validation data, reveal a clear win for our\\nproposed architecture. The Hax et al. [3] model has an average accuracy of 84.7%, while\\nours achieved 90.1%. Similarly, for test data, the Hax et al. [3] model achieved an accuracy\\nof 83.6%, while ours achieved an average accuracy of 89.7%.\\nTable 2. Average results based on the performance metrics for test and validation data.\\nArchitecture\\nModel\\nData\\nAccuracy\\nHax et al. [3]\\nInception-LSTM\\nValidation\\n84.7%\\nTest\\n83.6%\\nRes3-ATN [37]\\nResidual-Attention Network\\nValidation\\n64.13%\\nTest\\n62.70%'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='Spat. st. CNN [38]\\nTwo Streams CNN\\nValidation\\n55.79%\\nTest\\n54.60%\\nC3D [39]\\n3D CNNs\\nValidation\\n64.90%\\nTest\\n69.30%\\nOurs\\nROI-Inception-LSTM\\nValidation\\n90.1%\\nTest\\n89.7%\\nROI-Inception-GRU\\nValidation\\n85.6%\\nTest\\n84.1%\\nFigures 3 and 4 also present the classification confusion data for each category dataset.\\nThe true positive rate for each category is given on the diagonal squares in the confusion\\nmatrix. The most precise detected moment in validation is the “scroll_right” category.\\nElectronics 2024, 13, 3233\\n8 of 11\\nThe confusion matrix also gives a detailed insight into the true negatives, false positives,\\nand false negatives for each of the categories. By comparing these results with the baseline,\\nour proposed method demonstrates excellent performance, which can be attributed to our\\nmodification to the proposed architecture.\\nWe also compared the performance of our model with the state-of-the-art models.\\nThe classification results, as shown in Table 2, were quite satisfactory compared to other\\nmodels. To compare the computational complexity and memory usage of the proposed\\nmodel with the baseline methods, we have provided a comparison in Table 3. The results\\nclearly show the superiority of the proposed model in terms of weight and low memory\\nusage during inference.\\nTable 3. Comparison of model parameters and memory size.\\nModel\\nParameters\\nVRAM (MB)\\nHax et. al. [3]\\n29 M\\n450\\nRes3-ATN [37]\\n26 M\\n400\\nSpat. st CNN [38]\\n38 M\\n600\\nC3D [39]\\n25 M\\n350\\nOurs\\n16 M\\n150\\n5. Discussion\\nSection 4.2 is quite revealing in many ways, as it presents the detailed average results\\nin Tables 1 and 2 and the confusion matrix; the results show uniform error properties across\\nall categories. Gestures like “zoom_in” and “zoom_out” are primarily confused. Still, the\\ntrue positive rate is above 83%. The Inception-v3 combined with MediaPipe is a booster for\\nenhanced dynamic hand gesture classification. Even though the data were scarce, the model\\nperformance demonstrated that this algorithm can perform much better on sufficient data.\\nThe proposed model can also be used in other areas of HCI where temporal classification is\\ndemanded with minimal data availability for training. As discussed in the related work,\\nin the classification area using CNNs, the trend is to use a lightweight architecture that\\ncould achieve better results for practical application scenarios. Our proposed model paved\\nthe path for more lightweight architecture. It uses MediaPipe to extract the regions most\\nwanted from the image (ROI) and feed them to the middle layer for feature extraction. This\\napproach reduces the dimensionality of data and increases the availability of rich features;\\nthe LSTM layer further utilizes this for temporal data extraction and classification.\\nOur proposed model can also be used in lightweight real-world applications like\\ndrone control where real-time classification is needed. This is because drone control is\\nusually an outdoor activity, and the model can extract rich features with the power of\\nthe MediaPipe and Inception-v3 layer together, as well as being able to perform better\\nin the case of dynamic gestures when LSTM is used in the top layer as a classifier and\\ntemporal feature extractor. The deep learning models are always data-hungry, which is\\nwhy the current trend is to use transfer learning to overcome the issue of data scarcity.\\nUsing Resnet50, Inception-v3 is one of the many modules used in deep learning models to\\nmitigate the data scarcity issue; the proposed architecture can be used as a transfer learning\\nmodel in dynamic hand gesture recognition in various HCI applications.\\n6. Conclusions\\nIn this work, we proposed a hybrid deep learning architecture for classifying dynamic\\nhand gestures (video clips) for use in real-world scenarios. The architecture comprises\\nthree layers: The MediaPipe-based ROI extractor is the bottom layer, the middle layer\\nuses 2D CNN (Inception v3) as a feature extractor, and the top layer is a temporal feature\\nextractor and classifier. The proposed model was trained by the Depth_Camera_dataset,\\na dataset designed explicitly for dynamic hand gestures [36]. With our best-split data,'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='the model achieved an average test accuracy of more than 89.7%. This can be attributed\\nto our novel changes in the proposed hybrid architecture, which includes the addition of\\nElectronics 2024, 13, 3233\\n9 of 11\\nthe bottom layer, the MediaPipe-based hand region extractor (ROI), which reduces the\\ndimensionality and computational cost. The second reason is that the feature extractors\\nwere doubled; Inception v3 was used as a feature extractor in a focused area, enhancing the\\nfeatures’ quality while reducing the computational cost. The third change we made was\\nthe input to the LSTM layer, which concatenates features obtained for a sequence of frames\\nper gesture. In this study, we focused on limited gestures that were available in the dataset;\\nfurther investigations can be carried out on datasets with a large number of gestures. Also,\\nfor real world applications, diversity in the datasets is mandatory, as well as the subjective\\nevaluation of the model. Future work includes the subjective testing of the proposed\\narchitecture on other benchmark datasets and training the model on variable-length hand\\ngesture clips.\\nAuthor Contributions: Conceptualization, Y. and O.-J.K.; data curation, Y.; formal analysis, Y.;\\nfunding acquisition, O.-J.K., J.K. and J.L.; project administration, J.L.; resources, J.L.; software, Y., J.L.\\nand F.U.; supervision, O.-J.K.; visualization, Y.; writing—original draft, Y. and S.J.; writing—review\\nand editing, Y. and S.J. All authors have read and agreed to the published version of the manuscript.\\nFunding: This research was supported by Unmanned Vehicles Core Technology Research and\\nDevelopment Program through the National Research Foundation of Korea (NRF) and Unmanned\\nVehicle Advanced Research Center (UVARC) funded by the Ministry of Science and ICT, the Republic\\nof Korea (2023M3C1C1A01098414). This research was supported by the MSIT (Ministry of Science\\nand ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-\\n2024-2021-0-01816) supervised by the IITP (Institute for Information & Communications Technology\\nPlanning & Evaluation).\\nData Availability Statement: The raw data supporting the conclusions of this article will be made\\navailable by the authors on request.\\nConflicts of Interest: The authors declare no conflicts of interest.\\nAbbreviations\\nThe following abbreviations are used in this manuscript:\\nHGR\\nHand Gesture Recognition\\nCNN\\nConvolutional Neural Networks\\nRNN\\nRecurrent Neural Networks\\nGRU\\nGated Recurrent Unit\\nLSTM\\nLong Short-Term Memory\\nROI\\nRegion of Interest\\nReferences\\n1.\\nRastgoo, R.; Kiani, K.; Escalera, S.; Sabokrou, M. Multi-modal zero-shot dynamic hand gesture recognition. Expert Syst. Appl.\\n2024, 247, 123349. [CrossRef]\\n2.\\nBalaji, P.; Prusty, M.R. Multimodal fusion hierarchical self-attention network for dynamic hand gesture recognition. J. Vis.\\nCommun. Image Represent. 2024, 98, 104019. [CrossRef]\\n3.\\nHax, D.R.T.; Penava, P.; Krodel, S.; Razova, L.; Buettner, R. A Novel Hybrid Deep Learning Architecture for Dynamic Hand\\nGesture Recognition. IEEE Access 2024, 12, 28761–28774. [CrossRef]\\n4.\\nKarsh, B.; Laskar, R.H.; Karsh, R.K. mXception and dynamic image for hand gesture recognition. Neural Comput. Appl. 2024, 36,\\n8281. [CrossRef]\\n5.\\nSunanda; Balmik, A.; Nandy, A. A novel feature fusion technique for robust hand gesture recognition. Multimed. Tools Appl. 2024,\\n83, 65815–65831. [CrossRef]\\n6.\\nShi, Y.; Li, Y.; Fu, X.; Miao, K.; Miao, Q. Review of dynamic gesture recognition. Virtual Real. Intell. Hardw. 2021, 3, 183–206.\\n[CrossRef]\\n7.\\nJain, R.; Karsh, R.K.; Barbhuiya, A.A. Literature review of vision-based dynamic gesture recognition using deep learning'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='techniques. Concurr. Comput. Pract. Exp. 2022, 34, e7159. [CrossRef]\\n8.\\nKapuscinski, T.; Inglot, K. Vision-based gesture modeling for signed expressions recognition.\\nProcedia Comput. Sci. 2022,\\n207, 1007–1016. [CrossRef]\\n9.\\nYaseen; Kwon, O.J.; Lee, J.; Ullah, F.; Jamil, S.; Kim, J.S. Automatic Sequential Stitching of High-Resolution Panorama for Android\\nDevices Using Precapture Feature Detection and the Orientation Sensor. Sensors 2023, 23, 879. [CrossRef]\\nElectronics 2024, 13, 3233\\n10 of 11\\n10.\\nAbdullahi, S.B.; Chamnongthai, K. American sign language words recognition of skeletal videos using processed video driven\\nmulti-stacked deep LSTM. Sensors 2022, 22, 1406. [CrossRef] [PubMed]\\n11.\\nLeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature 2015, 521, 436–444. [CrossRef] [PubMed]\\n12.\\nSaxe, A.; Nelli, S.; Summerfield, C. If deep learning is the answer, what is the question? Nat. Rev. Neurosci. 2021, 22, 55–67.\\n[CrossRef] [PubMed]\\n13.\\nSimonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv 2014, arXiv:1409.1556.\\n14.\\nHe, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June–1 July 2016; pp. 770–778.\\n15.\\nSzegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going deeper\\nwith convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA,\\n7–12 June 2015; pp. 1–9.\\n16.\\nLi, Z.; Liu, F.; Yang, W.; Peng, S.; Zhou, J. A survey of convolutional neural networks: Analysis, applications, and prospects. IEEE\\nTrans. Neural Networks Learn. Syst. 2021, 33, 6999–7019. [CrossRef] [PubMed]\\n17.\\nWang, S.; Cao, J.; Philip, S.Y. Deep learning for spatio-temporal data mining: A survey. IEEE Trans. Knowl. Data Eng. 2020,\\n34, 3681–3700. [CrossRef]\\n18.\\nUr Rehman, A.; Belhaouari, S.B.; Kabir, M.A.; Khan, A. On the use of deep learning for video classification. Appl. Sci. 2023,\\n13, 2007. [CrossRef]\\n19.\\nAdithya, V.; Rajesh, R. A deep convolutional neural network approach for static hand gesture recognition. Procedia Comput. Sci.\\n2020, 171, 2353–2361.\\n20.\\nXia, C.; Saito, A.; Sugiura, Y. Using the virtual data-driven measurement to support the prototyping of hand gesture recognition\\ninterface with distance sensor. Sens. Actuators A Phys. 2022, 338, 113463. [CrossRef]\\n21.\\nDang, T.L.; Tran, S.D.; Nguyen, T.H.; Kim, S.; Monet, N. An improved hand gesture recognition system using keypoints and hand\\nbounding boxes. Array 2022, 16, 100251. [CrossRef]\\n22.\\nRautaray, S.S.; Agrawal, A. Real time gesture recognition system for interaction in dynamic environment. Procedia Technol. 2012,\\n4, 595–599. [CrossRef]\\n23.\\nNaguri, C.R.; Bunescu, R.C. Recognition of dynamic hand gestures from 3D motion data using LSTM and CNN architectures. In\\nProceedings of the 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), Cancun, Mexico,'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='18–21 December 2017; pp. 1130–1133.\\n24.\\nHuu, P.N.; Ngoc, T.L. Two-stream convolutional network for dynamic hand gesture recognition using convolutional long\\nshort-term memory networks. Vietnam J. Sci. Technol. 2020, 58, 514–523.\\n25.\\nZhang, W.; Wang, J. Dynamic hand gesture recognition based on 3D convolutional neural network models. In Proceedings of\\nthe 2019 IEEE 16th International Conference on Networking, Sensing and Control (ICNSC), Banff, AB, Canada, 9–11 May 2019;\\npp. 224–229.\\n26.\\nWang, X.; Lafreniere, B.; Zhao, J. Exploring Visualizations for Precisely Guiding Bare Hand Gestures in Virtual Reality. In\\nProceedings of the CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, 11–14 May 2024; pp. 1–19.\\n27.\\nYe, W.; Cheng, J.; Yang, F.; Xu, Y. Two-stream convolutional network for improving activity recognition using convolutional long\\nshort-term memory networks. IEEE Access 2019, 7, 67772–67780. [CrossRef]\\n28.\\nMa, C.Y.; Chen, M.H.; Kira, Z.; AlRegib, G. TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity\\nrecognition. Signal Process. Image Commun. 2019, 71, 76–87. [CrossRef]\\n29.\\nAbdullahi, S.B.; Bature, Z.A.; Gabralla, L.A.; Chiroma, H. Lie recognition with multi-modal spatial–temporal state transition\\npatterns based on hybrid convolutional neural network–bidirectional long short-term memory. Brain Sci. 2023, 13, 555. [CrossRef]\\n[PubMed]\\n30.\\nDurstewitz, D.; Koppe, G.; Thurm, M.I. Reconstructing computational system dynamics from neural data with recurrent neural\\nnetworks. Nat. Rev. Neurosci. 2023, 24, 693–710. [CrossRef] [PubMed]\\n31.\\nHendrikx, N.; Barhmi, K.; Visser, L.; de Bruin, T.; Pó, M.; Salah, A.; van Sark, W. All sky imaging-based short-term solar irradiance\\nforecasting with Long Short-Term Memory networks. Sol. Energy 2024, 272, 112463. [CrossRef]\\n32.\\nRafiq, I.; Mahmood, A.; Ahmed, U.; Khan, A.R.; Arshad, K.; Assaleh, K.; Ratyal, N.I.; Zoha, A. A Hybrid Approach for Forecasting\\nOccupancy of Building’s Multiple Space Types. IEEE Access 2024, 12, 50202–50216. [CrossRef]\\n33.\\nPan, Y.; Shang, Y.; Liu, T.; Shao, Z.; Guo, G.; Ding, H.; Hu, Q. Spatial–temporal attention network for depression recognition from\\nfacial videos. Expert Syst. Appl. 2024, 237, 121410. [CrossRef]\\n34.\\nSzegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; Wojna, Z. Rethinking the inception architecture for computer vision. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June–1 July 2016; pp. 2818–2826.\\n35.\\nLugaresi, C.; Tang, J.; Nash, H.; McClanahan, C.; Uboweja, E.; Hays, M.; Zhang, F.; Chang, C.L.; Yong, M.; Lee, J.; et al. Mediapipe:\\nA framework for perceiving and processing reality. In Proceedings of the Third Workshop on Computer Vision for AR/VR at\\nIEEE Computer Vision and Pattern Recognition (CVPR) 2019, Long Beach, CA, USA, 17 June 2019; Volume 2019.\\n36.\\nJeeru, S.; Sivapuram, A.K.; León, D.G.; Gröli, J.; Yeduri, S.R.; Cenkeramaddi, L.R. Depth camera based dataset of hand gestures.'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='Data Brief 2022, 45, 108659. [CrossRef]\\n37.\\nDhingra, N.; Kunz, A. Res3atn-Deep 3D residual attention network for hand gesture recognition in videos. In Proceedings of the\\n2019 International Conference on 3D Vision (3DV), Quebec City, QC, Canada, 16–19 September 2019; pp. 491–501.\\nElectronics 2024, 13, 3233\\n11 of 11\\n38.\\nSimonyan, K.; Zisserman, A. Two-stream convolutional networks for action recognition in videos. In Proceedings of the Advances\\nin Neural Information Processing Systems, Montreal, QC, Canada, 8–13 December 2014; Volume 27.\\n39.\\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; Paluri, M. Learning spatiotemporal features with 3D convolutional networks. In\\nProceedings of the IEEE International Conference on Computer Vision 2015, Santiago, Chile, 7–13 December 2015; pp. 4489–4497.\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VectorDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"llm\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='given number of test instances (1).\\n•\\nRecall: Also called true positive rate as it measures the model’s ability regarding true\\npositives (2).\\n•\\nF1-score: Reflects the model’s ability concerning false positives and negatives (3).\\n•\\nSpecificity: Reflects the model’s ability to correctly identify true negative instances (4).\\nFormulas for each of these metrics are given below:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN\\n(1)\\nRecall =\\nTP\\nTP + FN\\n(2)\\nF1-score = 2 × Precision × Recall\\nPrecision + Recall\\n(3)\\nSpecificity =\\nTN\\nTN + FP\\n(4)\\nElectronics 2024, 13, 3233\\n6 of 11\\nResults based on the above metrics are shown in Table 1. For more visually appealing\\nresults and deeper insights, we have used a confusion matrix as shown in Figures 3 and 4.\\nAlso, to further evaluate our results for the proposed architecture, we replace the\\nLSTM classifier with a gated recurrent unit (GRU) as these are also the most promising\\nmodels [18]. We consider the same data split for this architecture as well. We have also\\nused the Resnet50, replacing Inception v3 in our architecture, for further comparison.\\nFigure 3. Confusion matrix based on validation data.\\nFigure 4. Confusion matrix based on test data.\\nElectronics 2024, 13, 3233\\n7 of 11\\nTable 1. Average results based on the performance metrics for test and validation data.\\nData\\nRecall\\nF1-Score\\nSpecificity\\nAccuracy\\nValidation\\n89.9%\\n89.7%\\n98.0%\\n90.1%\\nTest\\n91.5%\\n90.0%\\n97.9%\\n89.7%\\n4. Results\\nThis Section presents the experimental setup, performance metrics, and visualization\\n(the reason and rationale behind our model architecture and dataset pre-processing).\\n4.1. Experimental Setup\\nThe models were trained for 300 epochs, with early stopping enabled to avoid over-\\nfitting, and the initial learning rate was set to 0.001. The categorical cross-entropy cost\\nfunction was used for training and validation losses for maximum learning. The adam-\\noptimizer was used to iteratively update and adjust the network weights in order to\\nminimize these losses. In the case of activation functions, the tangent hyperbolic function\\nwas used for the LSTM layer and the softmax function for the final output layer. For the\\nintermediate layer (Dense layer), the rectified linear unit (RelU) was employed (see Figure 2).\\nThe machine we used for our pipeline has the following specs: We used an HP desktop\\ncomputer, manufactured by HP Inc., Palo Alto, CA, USA, model Pavilion Gaming Desktop\\nTG01-1xxx, with a RAM of 32 GB, GPU of NVIDIA GeForce RTX 2060 SUPER, and eight\\ncores with clock speed of 2.9 GHz.\\n4.2. Performance Metrics and Visualizations\\nThe proposed hybrid deep learning-based model for hand gesture recognition demon-\\nstrated good performance when evaluated via various metrics, as shown in Table 1. The em-\\npirical tests, both on validation and test results, showcase satisfactory results; an average\\naccuracy of 89.7% on test data and 90.1% on validation data was reported. Other metrics\\nwere also used to assess the model’s ability; we achieved a recall of 91%, an F1-score of\\n90.0%, and a specificity of 98.0%.\\nThe results were also compared to that of other models [3,37–39], as shown in Table 2.\\nThe average accuracy on the test, as well as the validation data, reveal a clear win for our\\nproposed architecture. The Hax et al. [3] model has an average accuracy of 84.7%, while\\nours achieved 90.1%. Similarly, for test data, the Hax et al. [3] model achieved an accuracy\\nof 83.6%, while ours achieved an average accuracy of 89.7%.\\nTable 2. Average results based on the performance metrics for test and validation data.\\nArchitecture\\nModel\\nData\\nAccuracy\\nHax et al. [3]\\nInception-LSTM\\nValidation\\n84.7%\\nTest\\n83.6%\\nRes3-ATN [37]\\nResidual-Attention Network\\nValidation\\n64.13%\\nTest\\n62.70%'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='given number of test instances (1).\\n•\\nRecall: Also called true positive rate as it measures the model’s ability regarding true\\npositives (2).\\n•\\nF1-score: Reflects the model’s ability concerning false positives and negatives (3).\\n•\\nSpecificity: Reflects the model’s ability to correctly identify true negative instances (4).\\nFormulas for each of these metrics are given below:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN\\n(1)\\nRecall =\\nTP\\nTP + FN\\n(2)\\nF1-score = 2 × Precision × Recall\\nPrecision + Recall\\n(3)\\nSpecificity =\\nTN\\nTN + FP\\n(4)\\nElectronics 2024, 13, 3233\\n6 of 11\\nResults based on the above metrics are shown in Table 1. For more visually appealing\\nresults and deeper insights, we have used a confusion matrix as shown in Figures 3 and 4.\\nAlso, to further evaluate our results for the proposed architecture, we replace the\\nLSTM classifier with a gated recurrent unit (GRU) as these are also the most promising\\nmodels [18]. We consider the same data split for this architecture as well. We have also\\nused the Resnet50, replacing Inception v3 in our architecture, for further comparison.\\nFigure 3. Confusion matrix based on validation data.\\nFigure 4. Confusion matrix based on test data.\\nElectronics 2024, 13, 3233\\n7 of 11\\nTable 1. Average results based on the performance metrics for test and validation data.\\nData\\nRecall\\nF1-Score\\nSpecificity\\nAccuracy\\nValidation\\n89.9%\\n89.7%\\n98.0%\\n90.1%\\nTest\\n91.5%\\n90.0%\\n97.9%\\n89.7%\\n4. Results\\nThis Section presents the experimental setup, performance metrics, and visualization\\n(the reason and rationale behind our model architecture and dataset pre-processing).\\n4.1. Experimental Setup\\nThe models were trained for 300 epochs, with early stopping enabled to avoid over-\\nfitting, and the initial learning rate was set to 0.001. The categorical cross-entropy cost\\nfunction was used for training and validation losses for maximum learning. The adam-\\noptimizer was used to iteratively update and adjust the network weights in order to\\nminimize these losses. In the case of activation functions, the tangent hyperbolic function\\nwas used for the LSTM layer and the softmax function for the final output layer. For the\\nintermediate layer (Dense layer), the rectified linear unit (RelU) was employed (see Figure 2).\\nThe machine we used for our pipeline has the following specs: We used an HP desktop\\ncomputer, manufactured by HP Inc., Palo Alto, CA, USA, model Pavilion Gaming Desktop\\nTG01-1xxx, with a RAM of 32 GB, GPU of NVIDIA GeForce RTX 2060 SUPER, and eight\\ncores with clock speed of 2.9 GHz.\\n4.2. Performance Metrics and Visualizations\\nThe proposed hybrid deep learning-based model for hand gesture recognition demon-\\nstrated good performance when evaluated via various metrics, as shown in Table 1. The em-\\npirical tests, both on validation and test results, showcase satisfactory results; an average\\naccuracy of 89.7% on test data and 90.1% on validation data was reported. Other metrics\\nwere also used to assess the model’s ability; we achieved a recall of 91%, an F1-score of\\n90.0%, and a specificity of 98.0%.\\nThe results were also compared to that of other models [3,37–39], as shown in Table 2.\\nThe average accuracy on the test, as well as the validation data, reveal a clear win for our\\nproposed architecture. The Hax et al. [3] model has an average accuracy of 84.7%, while\\nours achieved 90.1%. Similarly, for test data, the Hax et al. [3] model achieved an accuracy\\nof 83.6%, while ours achieved an average accuracy of 89.7%.\\nTable 2. Average results based on the performance metrics for test and validation data.\\nArchitecture\\nModel\\nData\\nAccuracy\\nHax et al. [3]\\nInception-LSTM\\nValidation\\n84.7%\\nTest\\n83.6%\\nRes3-ATN [37]\\nResidual-Attention Network\\nValidation\\n64.13%\\nTest\\n62.70%'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the'),\n",
       " Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that you've provided a snippet of text from a research paper or article. The question is not explicitly stated, but I can try to infer what you might be looking for.\n",
      "\n",
      "Based on the content, it seems like you're interested in understanding the proposed changes and improvements made to a baseline architecture for hand gesture recognition using deep learning models. Specifically, you might want to know about:\n",
      "\n",
      "1. The use of MediaPipe as an ROI (Region of Interest) extractor.\n",
      "2. The comparison with Inception v3 alone.\n",
      "3. The reduction in computation complexity and cost.\n",
      "4. The proposed hybrid model consisting of three main layers: MediaPipe for ROI extraction, Inception v3 as a feature extractor, and LSTM for classification.\n",
      "\n",
      "If that's correct, I can try to provide a concise summary or answer your questions based on the provided text!\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hallucination Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "    Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question}\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Re-writer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like you want me to rewrite the initial question into a more optimized form for vector store retrieval, but the initial question itself isn\\'t provided.\\n\\nHowever, I can guide you on how to improve a question for better vector store retrieval:\\n\\n1. **Remove Preamble**: If your question starts with phrases like \"I was wondering,\" \"Can someone help me understand,\" or similar, remove them as they don\\'t add value to the query and might reduce its effectiveness in search.\\n\\n2. **Focus on Key Entities**: Identify key entities (people, places, things) that are crucial for understanding the question. These should be included in your revised question.\\n\\n3. **Use Specific Keywords**: Incorporate specific keywords related to the topic or subject of interest. This can help in retrieving more relevant information from a vector store.\\n\\n4. **Simplify and Clarify**: Simplify the language used while maintaining clarity. Avoid ambiguity by making sure what you\\'re asking is clear.\\n\\n5. **Avoid Negations**: If possible, rephrase questions that include negations (e.g., \"What didn\\'t happen\") to positive queries (e.g., \"What happened\").\\n\\n6. **Use Action Verbs**: Incorporate action verbs like \"find,\" \"show,\" or \"compare\" if they\\'re relevant and can help in specifying what you\\'re looking for.\\n\\n7. **Be Concise**: Keep your question concise while ensuring it still captures the essence of what you\\'re asking.\\n\\nGiven these guidelines, let\\'s assume a hypothetical initial question:\\n\\n**Initial Question:** \"I\\'m trying to find information about llm. Can someone explain how it works and its applications?\"\\n\\n**Improved Question:** \"What are the key features and uses of llm?\"\\n\\nThis revised question is more direct, includes specific keywords (\"llm\"), and focuses on the core query without unnecessary preamble or negations.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Prompt\n",
    "re_write_prompt = PromptTemplate(\n",
    "    template=\"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the initial and formulate an improved question. \\n\n",
    "     Here is the initial question: \\n\\n {question}. Improved question with no preamble: \\n \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    # Retrieve documents\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    # Generate answer\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "\n",
    "    # Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    # Transform the query to produce a better question.\n",
    "\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    # Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    # Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "{'documents': [Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='Data Brief 2022, 45, 108659. [CrossRef]\\n37.\\nDhingra, N.; Kunz, A. Res3atn-Deep 3D residual attention network for hand gesture recognition in videos. In Proceedings of the\\n2019 International Conference on 3D Vision (3DV), Quebec City, QC, Canada, 16–19 September 2019; pp. 491–501.\\nElectronics 2024, 13, 3233\\n11 of 11\\n38.\\nSimonyan, K.; Zisserman, A. Two-stream convolutional networks for action recognition in videos. In Proceedings of the Advances\\nin Neural Information Processing Systems, Montreal, QC, Canada, 8–13 December 2014; Volume 27.\\n39.\\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; Paluri, M. Learning spatiotemporal features with 3D convolutional networks. In\\nProceedings of the IEEE International Conference on Computer Vision 2015, Santiago, Chile, 7–13 December 2015; pp. 4489–4497.\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.'),\n",
      "               Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='Data Brief 2022, 45, 108659. [CrossRef]\\n37.\\nDhingra, N.; Kunz, A. Res3atn-Deep 3D residual attention network for hand gesture recognition in videos. In Proceedings of the\\n2019 International Conference on 3D Vision (3DV), Quebec City, QC, Canada, 16–19 September 2019; pp. 491–501.\\nElectronics 2024, 13, 3233\\n11 of 11\\n38.\\nSimonyan, K.; Zisserman, A. Two-stream convolutional networks for action recognition in videos. In Proceedings of the Advances\\nin Neural Information Processing Systems, Montreal, QC, Canada, 8–13 December 2014; Volume 27.\\n39.\\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; Paluri, M. Learning spatiotemporal features with 3D convolutional networks. In\\nProceedings of the IEEE International Conference on Computer Vision 2015, Santiago, Chile, 7–13 December 2015; pp. 4489–4497.\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.'),\n",
      "               Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the'),\n",
      "               Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the')],\n",
      " 'question': 'Summarise the key findings of the paper.'}\n",
      "\n",
      "---\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "{'documents': [Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the'),\n",
      "               Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the')],\n",
      " 'question': 'Summarise the key findings of the paper.'}\n",
      "\n",
      "---\n",
      "\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "Node 'generate':\n",
      "{'documents': [Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the'),\n",
      "               Document(metadata={'source': '/mnt/MainDrive/Codes/Heckeer/electronics-13-03233.pdf'}, page_content='leads to improved accuracy and lightweight models, reducing the computation complexity\\nand cost. The following points are presented for comparison:\\n•\\nThe use of MediaPipe as an ROI extractor effectively locates the ROI, reducing the\\nimage dimension and computational cost.\\n•\\nIt also offers improved efficiency for hand landmark detection compared to Inception\\nv3 alone.\\n•\\nTime complexity is low, and real-time performance is highly durable compared to\\nInception v3 without MediaPipe.\\n•\\nMediaPipe, when employed as an ROI extractor on the input layer in a hybrid architec-\\nture alongside an LSTM layer, as shown in Figure 1, exhibits a significant enhancement\\nin terms of classification accuracy.\\nComparing our architecture to the baseline, we have proposed the following changes:\\n•\\nWe have moved Google Inception v3 from the input layer to the middle layer [34]. The\\ninput layer uses Google MediaPipe as an ROI extractor [35].\\n•\\nThe frame dimensions are lowered from (1600 × 900 × 3) to (50 × 60 × 3).\\n•\\nIn the case of baseline architecture, ten frames are processed per sequence, and the\\nproposed architecture can process variable-length sequences.\\nThe processed data from MediaPipe is fed into the LSTM block as shown in Figure 2.\\nThe LSTM exploits the temporal relationship of the received data for classification. We\\nreplicate the same layer architecture as used by the authors [3], including dropout layers\\nand a dense layer (using a softmax function) at the final stage to prevent the model from\\nover-fitting and classification, respectively. The proposed hybrid model consists of three\\nmain layers. MediaPipe is used at the top layer to extract the ROI; in the middle layer,\\nGoogle Inception v3 is used as a feature extractor for input to the third layer, i.e., the LSTM\\nlayer, which is used as a classifier [18].\\n3.1. Dynamic Hand Gesture Dataset\\nFor a fair comparison of our proposed work with that of the baseline architecture, we\\nselect the same dataset, “Depth_Camera_Dataset” [36], for training the model, as used by\\nthe authors [3]. This dataset has been collected concerning different subjects under various\\nlighting conditions. It has six different gestures, each with 662 sequences, comprising 40\\nframes per sequence. The list of gestures is as follows: scroll down, scroll up, scroll left,\\nscroll right, zoom in, and zoom out. The background has additional people sometimes\\nengaging in other activities; this is included intentionally to add complexity to the dataset\\nfor real-world simulation. A depth camera recorded the dataset so that it also contains\\ndepth data, but our focus in this paper is a vision-based hybrid model.\\n3.2. Data Processing Pipeline\\nOur proposed deep learning-based hybrid architecture pipeline was implemented in\\nPython using TensorFlow. The proposed model consists of three main blocks: MediaPipe,\\nInception v3, and the final LSTM classifier, as shown in Figure 2. In the pre-processing\\nstage, frames from the directory are loaded sequence by sequence, and every fourth frame\\nis selected from the sequence, as more frames did not increase the accuracy [3]. Finally,\\na total of 10 frames are used per sequence. MediaPipe is a hand locator and a region of\\ninterest (ROI) extractor. The ROI is cropped for each frame. This technique removes the\\nElectronics 2024, 13, 3233\\n5 of 11\\nunwanted features from a given frame, thus reducing the dimensionality of the data (from\\n1600 × 900 × 3 to 50 × 60 × 3) per frame. Therefore, the pre-processed data frame from\\nthe MediaPipe block is (10, 50, 60, 3) in size. Inception v3 is used as a 2D CNN-based\\nfeature extractor, which then outputs a 1D row vector; combinedly, the size is (10, 2048,\\n1) per sequence. This data frame is then used as an input to the LSTM block for final\\nfeature extraction and classification. Also, 70, 20, and 10% of the data is split into training,\\nvalidation, and testing portions.\\nFigure 2. A triple-layer deep learning-based lightweight hybrid architecture.\\n3.3. Evaluating Criteria\\nFor the model’s assessment and evaluation, we used the following metrics:\\n•\\nAccuracy: Reflects the model’s performance regarding the overall correctness for the')],\n",
      " 'generation': 'The proposed hybrid model achieved improved accuracy in hand '\n",
      "               'gesture classification compared to the baseline architecture, '\n",
      "               'with a significant enhancement in terms of classification '\n",
      "               'accuracy. The use of MediaPipe as an ROI extractor effectively '\n",
      "               'located the ROI, reducing image dimension and computational '\n",
      "               'cost.',\n",
      " 'question': 'Summarise the key findings of the paper.'}\n",
      "\n",
      "---\n",
      "\n",
      "('The proposed hybrid model achieved improved accuracy in hand gesture '\n",
      " 'classification compared to the baseline architecture, with a significant '\n",
      " 'enhancement in terms of classification accuracy. The use of MediaPipe as an '\n",
      " 'ROI extractor effectively located the ROI, reducing image dimension and '\n",
      " 'computational cost.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"Summarise the key findings of the paper.\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "        pprint(value)\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
